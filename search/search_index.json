{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Jacob Rider jacob.rider34@gmail.com | LinkedIn | GitHub Professional Summary I am a math and data enthusiast seeking a position modeling complex data in order to inform decision making (with a little bit of help from generative AI). Throughout my career, I learned the extract, transform, and load (ETL) process using a variety of methods. In addition, my skillset includes dashboard creation and data visualization with softwares such as Python, R, Tableau, Power BI and Excel. These experiences uniquely position me to own the data modeling process from end-to-end. Skills: Python, SQL, R, AWS, Tableau, Power BI and Excel Passions: Generative AI, probabilistic programming and data modeling Soft Skills: Concise, empathetic and effective communication Hobbies: Board games, travel and food Experience | Education | Skills | Projects","title":"Home"},{"location":"#jacob-rider","text":"jacob.rider34@gmail.com | LinkedIn | GitHub","title":"Jacob Rider"},{"location":"#professional-summary","text":"I am a math and data enthusiast seeking a position modeling complex data in order to inform decision making (with a little bit of help from generative AI). Throughout my career, I learned the extract, transform, and load (ETL) process using a variety of methods. In addition, my skillset includes dashboard creation and data visualization with softwares such as Python, R, Tableau, Power BI and Excel. These experiences uniquely position me to own the data modeling process from end-to-end. Skills: Python, SQL, R, AWS, Tableau, Power BI and Excel Passions: Generative AI, probabilistic programming and data modeling Soft Skills: Concise, empathetic and effective communication Hobbies: Board games, travel and food Experience | Education | Skills | Projects","title":"Professional Summary"},{"location":"baseball_pitch_predictor/","text":"Baseball Pitch Predictor View Project on GitHub Project Overview Created a predictive model to analyze and forecast baseball pitch types based on historical pitch data. Technologies Used Python Jax Flax Imports import pandas as pd import numpy as np import jax import jax.numpy as jnp from flax import linen as nn from flax.training.train_state import TrainState import optax from jax.random import PRNGKey from parsePitchData import clean_data from sklearn.metrics import confusion_matrix import seaborn as sns import matplotlib.pyplot as plt from typing import Tuple, Dict, List import time Classes PitchPredictorModel class PitchPredictorModel(nn.Module): num_outputs: int # Number of unique pitch types @nn.compact def __call__(self, x: jnp.ndarray) -> jnp.ndarray: \"\"\" Forward pass for the pitch predictor model. Parameters: - x (jnp.ndarray): Input tensor with shape (batch_size, 4, n). Returns: - jnp.ndarray: Logits tensor with shape (batch_size, num_outputs). \"\"\" Functions normalize_counts def normalize_counts(pitch_counts: np.ndarray) -> np.ndarray: \"\"\" Normalize the pitch counts to get a distribution proportion. Parameters: - pitch_counts: Array of pitch counts. Returns: - np.ndarray: Normalized distribution of pitch counts. \"\"\" efficient_pitch_distribution def efficient_pitch_distribution(df: pd.DataFrame, pitch_types: List[str], filter_conditions: Dict[str, str]) -> np.ndarray: \"\"\"Calculate and normalize the distribution of pitch types, excluding the current event. Parameters: - df (pd.DataFrame): The dataframe containing pitch data. - pitch_types (List[str]): List of all possible pitch types. - filter_conditions (Dict[str, str]): Conditions to filter the dataframe. Returns: - np.ndarray: Normalized distribution of pitch counts. \"\"\" return_pitch_distributions def return_pitch_distributions(df: pd.DataFrame, df_row: pd.Series, pitch_types: List[str]) -> jnp.ndarray: \"\"\"Calculate pitch distributions for different contexts based on a row from the dataframe. Parameters: - df (pd.DataFrame): The dataframe containing pitch data. - df_row (pd.Series): A row from the dataframe specifying the current event. - pitch_types (List[str]): List of all possible pitch types. Returns: - jnp.ndarray: Stack of normalized pitch distributions for different contexts. \"\"\" return_current_pitch def return_current_pitch(df_row: pd.Series, pitch_types: List[str]) -> List[int]: \"\"\"Generate a one-hot encoded vector for the current pitch type. Parameters: - df_row (pd.Series): A row from the dataframe specifying the current event. - pitch_types (List[str]): List of all possible pitch types. Returns: - List[int]: One-hot encoded vector representing the current pitch type. \"\"\" clean_data() def clean_data() -> Tuple[jnp.ndarray, jnp.ndarray]: \"\"\"Load, preprocess data, and compute pitch distributions. Returns: - Tuple[jnp.ndarray, jnp.ndarray]: Tuple of inputs and outputs for modeling. \"\"\" cross_entropy_loss() def cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray: \"\"\" Computes the cross-entropy loss between logits and labels. Parameters: - logits (jnp.ndarray): Predictions from model. - labels (jnp.ndarray): True labels, one-hot encoded. Returns: - jnp.ndarray: Cross-entropy loss. \"\"\" compute_accuracy() def compute_accuracy(logits: jnp.ndarray, labels: jnp.ndarray) -> float: \"\"\" Computes the accuracy of the predictions. Parameters: - logits (jnp.ndarray): Predictions from model. - labels (jnp.ndarray): True labels, one-hot encoded. Returns: - float: Accuracy of predictions. \"\"\" create_train_state() def create_train_state(rng: PRNGKey, learning_rate: float, num_outputs: int) -> TrainState: \"\"\" Initializes the model's train state. Parameters: - rng (PRNGKey): Random number generator key. - learning_rate (float): Learning rate for the optimizer. - num_outputs (int): Number of unique pitch types. Returns: - TrainState: The initialized training state. \"\"\" train_step() def train_step(state: TrainState, batch: dict) -> tuple: \"\"\" Performs a single training step. Parameters: - state (TrainState): Current training state. - batch (dict): Batch of data containing 'inputs' and 'targets'. Returns: - tuple: Updated state, loss, and accuracy for the batch. \"\"\" train_model() def train_model(num_epochs: int, learning_rate: float, num_outputs: int, train_data: list) -> None: \"\"\" Trains the model. Parameters: - num_epochs (int): Number of epochs to train for. - learning_rate (float): Learning rate for the optimizer. - num_outputs (int): Number of unique pitch types. - train_data (list): Training data. \"\"\" Example Usage def main(): inputs, outputs = clean_data() # Assuming this returns correctly shaped data n = outputs.shape[1] learning_rate = 0.001 num_epochs = 10 class_weights = jnp.array([1.0,5.0,5.0,5.0]) # Example weights for 3 classes # Splitting data into training and testing sets num_training = int(0.8 * len(inputs)) train_inputs, test_inputs = inputs[:num_training], inputs[num_training:] train_outputs, test_outputs = outputs[:num_training], outputs[num_training:] # Initialize the model and training state rng = PRNGKey(0) state = create_train_state(rng, learning_rate, n) # Adjusted training loop for epoch in range(num_epochs): epoch_loss = 0 epoch_accuracy = 0 num_batches = len(train_inputs) # Assuming batch size of 1 for simplicity for i in range(num_batches): inputs = train_inputs[i].reshape(1, 4, n) outputs = train_outputs[i].reshape(1, n) state, loss, accuracy = train_step(state, {'inputs': inputs, 'targets': outputs}) epoch_loss += loss epoch_accuracy += accuracy epoch_loss /= num_batches epoch_accuracy /= num_batches print(f\"Epoch {epoch}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\") # Initialize a list to store prediction logits test_pred_logits = [] # Predict in a loop for i in range(len(test_inputs)): # Reshape the input correctly as the model expects test_input_reshaped = test_inputs[i].reshape(1, 4, n) # Correctly use the trained parameters for prediction logits = state.apply_fn({'params': state.params}, test_input_reshaped) test_pred_logits.append(logits) # Assuming test_pred_logits is now a list of arrays, stack them test_pred_logits = jnp.vstack(test_pred_logits) # Stack logits for further processing # Convert logits to predicted class indices test_pred_labels = jnp.argmax(test_pred_logits, axis=-1) # Ensure true_labels is correctly prepared from test_outputs true_labels = jnp.argmax(test_outputs, axis=-1) # Compute the confusion matrix cm = confusion_matrix(true_labels, test_pred_labels) # Plot the confusion matrix plt.figure(figsize=(10, 7)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') plt.xlabel('Predicted Labels') plt.ylabel('True Labels') plt.title('Confusion Matrix') plt.savefig('plots/prediction_results1.png') if __name__ == \"__main__\": main() Experience | Education | Skills | Projects TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Baseball Pitch Predictor"},{"location":"baseball_pitch_predictor/#baseball-pitch-predictor","text":"View Project on GitHub","title":"Baseball Pitch Predictor"},{"location":"baseball_pitch_predictor/#project-overview","text":"Created a predictive model to analyze and forecast baseball pitch types based on historical pitch data.","title":"Project Overview"},{"location":"baseball_pitch_predictor/#technologies-used","text":"Python Jax Flax","title":"Technologies Used"},{"location":"baseball_pitch_predictor/#imports","text":"import pandas as pd import numpy as np import jax import jax.numpy as jnp from flax import linen as nn from flax.training.train_state import TrainState import optax from jax.random import PRNGKey from parsePitchData import clean_data from sklearn.metrics import confusion_matrix import seaborn as sns import matplotlib.pyplot as plt from typing import Tuple, Dict, List import time","title":"Imports"},{"location":"baseball_pitch_predictor/#classes","text":"","title":"Classes"},{"location":"baseball_pitch_predictor/#pitchpredictormodel","text":"class PitchPredictorModel(nn.Module): num_outputs: int # Number of unique pitch types @nn.compact def __call__(self, x: jnp.ndarray) -> jnp.ndarray: \"\"\" Forward pass for the pitch predictor model. Parameters: - x (jnp.ndarray): Input tensor with shape (batch_size, 4, n). Returns: - jnp.ndarray: Logits tensor with shape (batch_size, num_outputs). \"\"\"","title":"PitchPredictorModel"},{"location":"baseball_pitch_predictor/#functions","text":"","title":"Functions"},{"location":"baseball_pitch_predictor/#normalize_counts","text":"def normalize_counts(pitch_counts: np.ndarray) -> np.ndarray: \"\"\" Normalize the pitch counts to get a distribution proportion. Parameters: - pitch_counts: Array of pitch counts. Returns: - np.ndarray: Normalized distribution of pitch counts. \"\"\"","title":"normalize_counts"},{"location":"baseball_pitch_predictor/#efficient_pitch_distribution","text":"def efficient_pitch_distribution(df: pd.DataFrame, pitch_types: List[str], filter_conditions: Dict[str, str]) -> np.ndarray: \"\"\"Calculate and normalize the distribution of pitch types, excluding the current event. Parameters: - df (pd.DataFrame): The dataframe containing pitch data. - pitch_types (List[str]): List of all possible pitch types. - filter_conditions (Dict[str, str]): Conditions to filter the dataframe. Returns: - np.ndarray: Normalized distribution of pitch counts. \"\"\"","title":"efficient_pitch_distribution"},{"location":"baseball_pitch_predictor/#return_pitch_distributions","text":"def return_pitch_distributions(df: pd.DataFrame, df_row: pd.Series, pitch_types: List[str]) -> jnp.ndarray: \"\"\"Calculate pitch distributions for different contexts based on a row from the dataframe. Parameters: - df (pd.DataFrame): The dataframe containing pitch data. - df_row (pd.Series): A row from the dataframe specifying the current event. - pitch_types (List[str]): List of all possible pitch types. Returns: - jnp.ndarray: Stack of normalized pitch distributions for different contexts. \"\"\"","title":"return_pitch_distributions"},{"location":"baseball_pitch_predictor/#return_current_pitch","text":"def return_current_pitch(df_row: pd.Series, pitch_types: List[str]) -> List[int]: \"\"\"Generate a one-hot encoded vector for the current pitch type. Parameters: - df_row (pd.Series): A row from the dataframe specifying the current event. - pitch_types (List[str]): List of all possible pitch types. Returns: - List[int]: One-hot encoded vector representing the current pitch type. \"\"\"","title":"return_current_pitch"},{"location":"baseball_pitch_predictor/#clean_data","text":"def clean_data() -> Tuple[jnp.ndarray, jnp.ndarray]: \"\"\"Load, preprocess data, and compute pitch distributions. Returns: - Tuple[jnp.ndarray, jnp.ndarray]: Tuple of inputs and outputs for modeling. \"\"\"","title":"clean_data()"},{"location":"baseball_pitch_predictor/#cross_entropy_loss","text":"def cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray: \"\"\" Computes the cross-entropy loss between logits and labels. Parameters: - logits (jnp.ndarray): Predictions from model. - labels (jnp.ndarray): True labels, one-hot encoded. Returns: - jnp.ndarray: Cross-entropy loss. \"\"\"","title":"cross_entropy_loss()"},{"location":"baseball_pitch_predictor/#compute_accuracy","text":"def compute_accuracy(logits: jnp.ndarray, labels: jnp.ndarray) -> float: \"\"\" Computes the accuracy of the predictions. Parameters: - logits (jnp.ndarray): Predictions from model. - labels (jnp.ndarray): True labels, one-hot encoded. Returns: - float: Accuracy of predictions. \"\"\"","title":"compute_accuracy()"},{"location":"baseball_pitch_predictor/#create_train_state","text":"def create_train_state(rng: PRNGKey, learning_rate: float, num_outputs: int) -> TrainState: \"\"\" Initializes the model's train state. Parameters: - rng (PRNGKey): Random number generator key. - learning_rate (float): Learning rate for the optimizer. - num_outputs (int): Number of unique pitch types. Returns: - TrainState: The initialized training state. \"\"\"","title":"create_train_state()"},{"location":"baseball_pitch_predictor/#train_step","text":"def train_step(state: TrainState, batch: dict) -> tuple: \"\"\" Performs a single training step. Parameters: - state (TrainState): Current training state. - batch (dict): Batch of data containing 'inputs' and 'targets'. Returns: - tuple: Updated state, loss, and accuracy for the batch. \"\"\"","title":"train_step()"},{"location":"baseball_pitch_predictor/#train_model","text":"def train_model(num_epochs: int, learning_rate: float, num_outputs: int, train_data: list) -> None: \"\"\" Trains the model. Parameters: - num_epochs (int): Number of epochs to train for. - learning_rate (float): Learning rate for the optimizer. - num_outputs (int): Number of unique pitch types. - train_data (list): Training data. \"\"\"","title":"train_model()"},{"location":"baseball_pitch_predictor/#example-usage","text":"def main(): inputs, outputs = clean_data() # Assuming this returns correctly shaped data n = outputs.shape[1] learning_rate = 0.001 num_epochs = 10 class_weights = jnp.array([1.0,5.0,5.0,5.0]) # Example weights for 3 classes # Splitting data into training and testing sets num_training = int(0.8 * len(inputs)) train_inputs, test_inputs = inputs[:num_training], inputs[num_training:] train_outputs, test_outputs = outputs[:num_training], outputs[num_training:] # Initialize the model and training state rng = PRNGKey(0) state = create_train_state(rng, learning_rate, n) # Adjusted training loop for epoch in range(num_epochs): epoch_loss = 0 epoch_accuracy = 0 num_batches = len(train_inputs) # Assuming batch size of 1 for simplicity for i in range(num_batches): inputs = train_inputs[i].reshape(1, 4, n) outputs = train_outputs[i].reshape(1, n) state, loss, accuracy = train_step(state, {'inputs': inputs, 'targets': outputs}) epoch_loss += loss epoch_accuracy += accuracy epoch_loss /= num_batches epoch_accuracy /= num_batches print(f\"Epoch {epoch}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\") # Initialize a list to store prediction logits test_pred_logits = [] # Predict in a loop for i in range(len(test_inputs)): # Reshape the input correctly as the model expects test_input_reshaped = test_inputs[i].reshape(1, 4, n) # Correctly use the trained parameters for prediction logits = state.apply_fn({'params': state.params}, test_input_reshaped) test_pred_logits.append(logits) # Assuming test_pred_logits is now a list of arrays, stack them test_pred_logits = jnp.vstack(test_pred_logits) # Stack logits for further processing # Convert logits to predicted class indices test_pred_labels = jnp.argmax(test_pred_logits, axis=-1) # Ensure true_labels is correctly prepared from test_outputs true_labels = jnp.argmax(test_outputs, axis=-1) # Compute the confusion matrix cm = confusion_matrix(true_labels, test_pred_labels) # Plot the confusion matrix plt.figure(figsize=(10, 7)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') plt.xlabel('Predicted Labels') plt.ylabel('True Labels') plt.title('Confusion Matrix') plt.savefig('plots/prediction_results1.png') if __name__ == \"__main__\": main() Experience | Education | Skills | Projects TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Example Usage"},{"location":"data_tools/","text":"Data Tools for Economic Research View Project on GitHub Project Overview Developed a suite of data analysis tools tailored to economic research, facilitating the efficient cleaning, processing, and ingestion of economic and financial API data into a SQLite database. Key Features Automated data cleaning and processing workflows Ingestion of diverse economic and financial data into a unified database Simplified access to clean, processed data for economic research and analysis Technologies Used Python SQL Imports import sqlite3 import requests import pandas as pd from sklearn.preprocessing import StandardScaler from typing import List, Optional Classes DataIngestor class DataIngestor: \"\"\" A class to ingest and process financial data from a SQLite database. \"\"\" def __init__(self, db_path: str, table_names: list, min_date: str): \"\"\" Initialize the DataIngestor with database path, table names, and minimum date. Args: db_path (str): Path to the SQLite database. table_names (list): List of table names to process. min_date (str): Minimum date for filtering data. \"\"\" self.db_path = db_path self.table_names = table_names self.min_date = min_date DataFetcher class DataFetcher: \"\"\" A class to fetch and insert financial and economic data into a SQLite database. Attributes: db_path (str): The file path for the SQLite database. \"\"\" def __init__(self, db_path: str) -> None: \"\"\" Initialize the DataFetcher class. Args: db_path (str): The file path for the SQLite database. \"\"\" self.db_path = db_path Methods DataFetcher Methods create_table() def create_table(self, table_name: str, columns: List[str], is_stock: bool) -> None: \"\"\" Create a table in the SQLite database. Args: table_name (str): The name of the table to create. columns (List[str]): A list of column names for the table. isStock (bool): Indicator of whether the data is stock data (True) or economic data (False). \"\"\" fetch_and_insert_stock_data() def fetch_and_insert_stock_data( self, stock_ticker: str, api_key: str, report_link: str, from_date: str, to_date: str, ) -> None: \"\"\" Fetch stock data from an API and insert it into the database. Args: stock_ticker (str): The stock ticker symbol. api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. from_date (str): Start date for the data. to_date (str): End date for the data. \"\"\" fetch_and_insert_economic_data() def fetch_and_insert_economic_data(self, api_key: str, report_link: str) -> None: \"\"\" Fetch economic data from an API and insert it into the database. Args: api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. \"\"\" _insert_data() def _insert_data( self, table_name: str, columns: List[str], data: List[dict], ticker: Optional[str] = None, ) -> None: \"\"\" Insert data into a specified table in the database. Args: table_name (str): The name of the table to insert data into. columns (List[str]): The column names for data insertion. data (List[Dict]): The data to be inserted. ticker (Optional[str]): The stock ticker symbol. Defaults to None. \"\"\" DataIngestor Methods get_column_names() def get_column_names(self, conn: sqlite3.Connection, table_name: str) -> list: \"\"\" Retrieve column names for a given table in the database. Args: conn (sqlite3.Connection): A connection object to the SQLite database. table_name (str): Name of the table to retrieve columns from. Returns: list: A list of column names. \"\"\" fetch_and_process_data() def fetch_and_process_data(self) -> list: \"\"\" Fetch and process data from the database for each table. Returns: list: A list of processed pandas DataFrames. \"\"\" scale_data() def scale_data(self, dfs: list) -> pd.DataFrame: \"\"\" Scale the data using StandardScaler. Args: dfs (list): List of pandas DataFrames to scale. Returns: pd.DataFrame: A DataFrame of scaled features. \"\"\" utils process_data() def process_data(db_path: str, table_names: list, min_date: str) -> tuple: \"\"\" Process and scale data from the database. Args: db_path (str): Path to the SQLite database. table_names (list): List of table names to process. min_date (str): Minimum date for filtering data. Returns: tuple: A tuple containing list of DataFrames and scaled DataFrame. \"\"\" drop_table() def drop_table(db_path: str, table_name: str) -> None: \"\"\" Drop a table from the database. Args: db_path (str): Path to the SQLite database. table_name (str): Name of the table to be dropped. \"\"\" fetch_stocks_data() def fetch_stocks_data( db_path: str, stocks: List[str], api_key: str, report_link: str ) -> None: \"\"\" Fetch and insert stock data for multiple stocks into the database. Args: db_path (str): The file path for the SQLite database. stocks (List[str]): A list of stock ticker symbols. api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. fetch_economic_data() def fetch_economic_data(db_path: str, table_names: List[str], api_key: str) -> None: \"\"\" Fetch and insert economic data for multiple series into the database. Args: db_path (str): The file path for the SQLite database. table_names (list[str]): A list of economic data series identifiers. api_key (str): The API key for authentication. \"\"\" Experience | Education | Skills | Projects TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Data Tools for Economic Research"},{"location":"data_tools/#data-tools-for-economic-research","text":"View Project on GitHub","title":"Data Tools for Economic Research"},{"location":"data_tools/#project-overview","text":"Developed a suite of data analysis tools tailored to economic research, facilitating the efficient cleaning, processing, and ingestion of economic and financial API data into a SQLite database.","title":"Project Overview"},{"location":"data_tools/#key-features","text":"Automated data cleaning and processing workflows Ingestion of diverse economic and financial data into a unified database Simplified access to clean, processed data for economic research and analysis","title":"Key Features"},{"location":"data_tools/#technologies-used","text":"Python SQL","title":"Technologies Used"},{"location":"data_tools/#imports","text":"import sqlite3 import requests import pandas as pd from sklearn.preprocessing import StandardScaler from typing import List, Optional","title":"Imports"},{"location":"data_tools/#classes","text":"","title":"Classes"},{"location":"data_tools/#dataingestor","text":"class DataIngestor: \"\"\" A class to ingest and process financial data from a SQLite database. \"\"\" def __init__(self, db_path: str, table_names: list, min_date: str): \"\"\" Initialize the DataIngestor with database path, table names, and minimum date. Args: db_path (str): Path to the SQLite database. table_names (list): List of table names to process. min_date (str): Minimum date for filtering data. \"\"\" self.db_path = db_path self.table_names = table_names self.min_date = min_date","title":"DataIngestor"},{"location":"data_tools/#datafetcher","text":"class DataFetcher: \"\"\" A class to fetch and insert financial and economic data into a SQLite database. Attributes: db_path (str): The file path for the SQLite database. \"\"\" def __init__(self, db_path: str) -> None: \"\"\" Initialize the DataFetcher class. Args: db_path (str): The file path for the SQLite database. \"\"\" self.db_path = db_path","title":"DataFetcher"},{"location":"data_tools/#methods","text":"","title":"Methods"},{"location":"data_tools/#datafetcher-methods","text":"","title":"DataFetcher Methods"},{"location":"data_tools/#create_table","text":"def create_table(self, table_name: str, columns: List[str], is_stock: bool) -> None: \"\"\" Create a table in the SQLite database. Args: table_name (str): The name of the table to create. columns (List[str]): A list of column names for the table. isStock (bool): Indicator of whether the data is stock data (True) or economic data (False). \"\"\"","title":"create_table()"},{"location":"data_tools/#fetch_and_insert_stock_data","text":"def fetch_and_insert_stock_data( self, stock_ticker: str, api_key: str, report_link: str, from_date: str, to_date: str, ) -> None: \"\"\" Fetch stock data from an API and insert it into the database. Args: stock_ticker (str): The stock ticker symbol. api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. from_date (str): Start date for the data. to_date (str): End date for the data. \"\"\"","title":"fetch_and_insert_stock_data()"},{"location":"data_tools/#fetch_and_insert_economic_data","text":"def fetch_and_insert_economic_data(self, api_key: str, report_link: str) -> None: \"\"\" Fetch economic data from an API and insert it into the database. Args: api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. \"\"\"","title":"fetch_and_insert_economic_data()"},{"location":"data_tools/#_insert_data","text":"def _insert_data( self, table_name: str, columns: List[str], data: List[dict], ticker: Optional[str] = None, ) -> None: \"\"\" Insert data into a specified table in the database. Args: table_name (str): The name of the table to insert data into. columns (List[str]): The column names for data insertion. data (List[Dict]): The data to be inserted. ticker (Optional[str]): The stock ticker symbol. Defaults to None. \"\"\"","title":"_insert_data()"},{"location":"data_tools/#dataingestor-methods","text":"","title":"DataIngestor Methods"},{"location":"data_tools/#get_column_names","text":"def get_column_names(self, conn: sqlite3.Connection, table_name: str) -> list: \"\"\" Retrieve column names for a given table in the database. Args: conn (sqlite3.Connection): A connection object to the SQLite database. table_name (str): Name of the table to retrieve columns from. Returns: list: A list of column names. \"\"\"","title":"get_column_names()"},{"location":"data_tools/#fetch_and_process_data","text":"def fetch_and_process_data(self) -> list: \"\"\" Fetch and process data from the database for each table. Returns: list: A list of processed pandas DataFrames. \"\"\"","title":"fetch_and_process_data()"},{"location":"data_tools/#scale_data","text":"def scale_data(self, dfs: list) -> pd.DataFrame: \"\"\" Scale the data using StandardScaler. Args: dfs (list): List of pandas DataFrames to scale. Returns: pd.DataFrame: A DataFrame of scaled features. \"\"\"","title":"scale_data()"},{"location":"data_tools/#utils","text":"","title":"utils"},{"location":"data_tools/#process_data","text":"def process_data(db_path: str, table_names: list, min_date: str) -> tuple: \"\"\" Process and scale data from the database. Args: db_path (str): Path to the SQLite database. table_names (list): List of table names to process. min_date (str): Minimum date for filtering data. Returns: tuple: A tuple containing list of DataFrames and scaled DataFrame. \"\"\"","title":"process_data()"},{"location":"data_tools/#drop_table","text":"def drop_table(db_path: str, table_name: str) -> None: \"\"\" Drop a table from the database. Args: db_path (str): Path to the SQLite database. table_name (str): Name of the table to be dropped. \"\"\"","title":"drop_table()"},{"location":"data_tools/#fetch_stocks_data","text":"def fetch_stocks_data( db_path: str, stocks: List[str], api_key: str, report_link: str ) -> None: \"\"\" Fetch and insert stock data for multiple stocks into the database. Args: db_path (str): The file path for the SQLite database. stocks (List[str]): A list of stock ticker symbols. api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request.","title":"fetch_stocks_data()"},{"location":"data_tools/#fetch_economic_data","text":"def fetch_economic_data(db_path: str, table_names: List[str], api_key: str) -> None: \"\"\" Fetch and insert economic data for multiple series into the database. Args: db_path (str): The file path for the SQLite database. table_names (list[str]): A list of economic data series identifiers. api_key (str): The API key for authentication. \"\"\" Experience | Education | Skills | Projects TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"fetch_economic_data()"},{"location":"education/","text":"Bachelors of Science Mathematics and Economics Georgia State University, Atlanta, GA | Class of 2020 Completed a comprehensive curriculum focusing on rigorous math courses, quantitative analysis, statistical methods, and economic theory Managed a team project with the objective of modeling asset bubbles, utilizing Bloomberg data and physics equations Developed experimental data visualizations for the Molkov Theoretical Neuroscience Research Group in the RIMMES program. Modeled the feedback mechanisms that underlie the cardio-respiratory system using Seaborn and Python Proofread the team\u2019s principal research paper to ensure the data accuracy and readability of the following publication: E.M. Latash, W.H. Barnett, H. Park, J.M. Rider, A.N. Klishko, B.I. Prilutsky, Y.I. Molkov (2020) Frontal plane dynamics of the centre of mass during quadrupedal locomotion on a split-belt treadmill. J R Soc Interface. 2020;17(170):20200547. doi:10.1098/rsif.2020.05 Relevant Courses MATH 8525 | Applied Stochastic Processes | Probabilistic Programming Tools STAT 8674 | Monte Carlo Methods | TI4 Combat Simulator | MCMC with Efficient Jumps MATH 4991 | Senior Seminar (Math) | Nucleation of Asset Bubbles | Complex Exponential Map MATH 4751 | Mathematical Statistics I MATH 4010 | Mathematical Biology ECON 4751 | Intro to Game Theory ECON 4150 | Theory of Risk ECON 4950 | Econometrics & Applications ECON 4930 | Mathematical Economics MATH 2652/4265 | Ordinary/Partial Differential Equations MATH 2641/4435 | Linear Algebra I/II MATH 4441/4442 | Modern Algebra I/II MATH 4661/4662 | Real Analysis I/II MATH 4250 | Complex Analysis Certifications Tableau Data Analyst Tableau Desktop Specialist Experience | Education | Skills | Projects","title":"Education"},{"location":"education/#bachelors-of-science","text":"","title":"Bachelors of Science"},{"location":"education/#mathematics-and-economics","text":"Georgia State University, Atlanta, GA | Class of 2020 Completed a comprehensive curriculum focusing on rigorous math courses, quantitative analysis, statistical methods, and economic theory Managed a team project with the objective of modeling asset bubbles, utilizing Bloomberg data and physics equations Developed experimental data visualizations for the Molkov Theoretical Neuroscience Research Group in the RIMMES program. Modeled the feedback mechanisms that underlie the cardio-respiratory system using Seaborn and Python Proofread the team\u2019s principal research paper to ensure the data accuracy and readability of the following publication: E.M. Latash, W.H. Barnett, H. Park, J.M. Rider, A.N. Klishko, B.I. Prilutsky, Y.I. Molkov (2020) Frontal plane dynamics of the centre of mass during quadrupedal locomotion on a split-belt treadmill. J R Soc Interface. 2020;17(170):20200547. doi:10.1098/rsif.2020.05","title":"Mathematics and Economics"},{"location":"education/#relevant-courses","text":"MATH 8525 | Applied Stochastic Processes | Probabilistic Programming Tools STAT 8674 | Monte Carlo Methods | TI4 Combat Simulator | MCMC with Efficient Jumps MATH 4991 | Senior Seminar (Math) | Nucleation of Asset Bubbles | Complex Exponential Map MATH 4751 | Mathematical Statistics I MATH 4010 | Mathematical Biology ECON 4751 | Intro to Game Theory ECON 4150 | Theory of Risk ECON 4950 | Econometrics & Applications ECON 4930 | Mathematical Economics MATH 2652/4265 | Ordinary/Partial Differential Equations MATH 2641/4435 | Linear Algebra I/II MATH 4441/4442 | Modern Algebra I/II MATH 4661/4662 | Real Analysis I/II MATH 4250 | Complex Analysis","title":"Relevant Courses"},{"location":"education/#certifications","text":"Tableau Data Analyst Tableau Desktop Specialist Experience | Education | Skills | Projects","title":"Certifications"},{"location":"experience/","text":"Delta Air Lines Associate Data Engineer Atlanta, GA | June 2022 \u2013 Present Supported business intelligence initiatives through the development, maintenance, and scaling of data pipelines and ETL processes Collaborated cross functionally to understand data needs while spearheading the implementation of Tableau dashboards Led the coordination with key stakeholders to identify and translate business needs into technical requirements in order to ensure the successful alignment of data engineering projects with organizational objectives. Played a key role in the migration of legacy systems to AWS, ensuring minimal downtime and data integrity Mercedes-Benz USA, LLC Logistics Document Specialist Atlanta, GA | March 2021 \u2013 December 2021 Created database queries to automatically generate KPIs to drive monthly carrier performance reviews Enhanced data accuracy and minimized expenses by crafting a Python script for email parsing, effectively consolidating Vehicle Identification Numbers (VINs) and Purchase Order (PO) numbers Slashed 50 hours per month from crucial stakeholders' schedules by automating recurring management reports using a combination of Excel, VBA, SAP, and Python scripts Guaranteed uniformity and precision in documentation throughout the organization by creating comprehensive training materials and standard operating procedures (SOPs) Experience | Education | Skills | Projects","title":"Experience"},{"location":"experience/#delta-air-lines","text":"","title":"Delta Air Lines"},{"location":"experience/#associate-data-engineer","text":"Atlanta, GA | June 2022 \u2013 Present Supported business intelligence initiatives through the development, maintenance, and scaling of data pipelines and ETL processes Collaborated cross functionally to understand data needs while spearheading the implementation of Tableau dashboards Led the coordination with key stakeholders to identify and translate business needs into technical requirements in order to ensure the successful alignment of data engineering projects with organizational objectives. Played a key role in the migration of legacy systems to AWS, ensuring minimal downtime and data integrity","title":"Associate Data Engineer"},{"location":"experience/#mercedes-benz-usa-llc","text":"","title":"Mercedes-Benz USA, LLC"},{"location":"experience/#logistics-document-specialist","text":"Atlanta, GA | March 2021 \u2013 December 2021 Created database queries to automatically generate KPIs to drive monthly carrier performance reviews Enhanced data accuracy and minimized expenses by crafting a Python script for email parsing, effectively consolidating Vehicle Identification Numbers (VINs) and Purchase Order (PO) numbers Slashed 50 hours per month from crucial stakeholders' schedules by automating recurring management reports using a combination of Excel, VBA, SAP, and Python scripts Guaranteed uniformity and precision in documentation throughout the organization by creating comprehensive training materials and standard operating procedures (SOPs) Experience | Education | Skills | Projects","title":"Logistics Document Specialist"},{"location":"prob_prog_ex/","text":"Probabilistic Programming Examples View Project on GitHub Project Overview Created several introductory examples using NumPyro including a model of the lorenz system. Technologies Used Python Jax NumPyro Matplotlib Imports import argparse import jax import jax.numpy as jnp from jax import random, jit from jax.experimental.ode import odeint import matplotlib import matplotlib.pyplot as plt import seaborn as sns import numpy as np import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC, NUTS, Predictive import arviz as az import os from typing import Optional, Sequence Example 1 - Coinflip Coinflip Posterior Plot def coin_flip_model(observed_data: jnp.ndarray) -> None: \"\"\" Bayesian model for coin flipping using a Bernoulli distribution. Parameters: - observed_data: jnp.ndarray, array of observed coin flips (0s and 1s). \"\"\" prob_heads = numpyro.sample(\"prob_heads\", dist.Uniform(0, 1)) numpyro.sample(\"obs\", dist.Bernoulli(prob_heads), obs=observed_data) def run_mcmc(model: callable, data: jnp.ndarray, num_warmup: int = 1000, num_samples: int = 1000, num_chains: int = 1) -> MCMC: \"\"\" Runs MCMC for a given Bayesian model and observed data. Parameters: - model: Callable, the Bayesian model function. - data: jnp.ndarray, the observed data to fit the model to. - num_warmup: int, number of warmup steps. - num_samples: int, number of samples to draw. - num_chains: int, number of MCMC chains to run. Returns: - MCMC object after running the MCMC algorithm. \"\"\" nuts_kernel = NUTS(model) mcmc = MCMC(nuts_kernel, num_warmup=num_warmup, num_samples=num_samples, num_chains=num_chains) mcmc.run(jax.random.PRNGKey(0), observed_data=data) return mcmc def main(): # Generate observed data for a biased coin key = random.PRNGKey(0) prob_heads = 0.7 num_flips = 100 observed_data = (random.uniform(key, (num_flips,)) < prob_heads).astype(int) # Run MCMC mcmc = run_mcmc(coin_flip_model, observed_data) # Convert to ArviZ InferenceData idata = az.from_numpyro(mcmc) # Plotting the posterior ax = az.plot_posterior(idata, var_names=[\"prob_heads\"]) # Save the figure plt.savefig(\"coin_flip_posterior_plot.pdf\") if __name__ == \"__main__\": main() Example 2 - Linear Regression Linear Regression Posterior Plot # True parameters for generating synthetic data true_slope: float = 2.5 true_intercept: float = -1.0 noise_scale: float = 1.0 # Generate synthetic data np.random.seed(0) x: np.ndarray = np.linspace(0, 1, 100) noise: np.ndarray = np.random.normal(scale=noise_scale, size=x.shape) y: np.ndarray = true_slope * x + true_intercept + noise def linear_regression_model(x: np.ndarray, y: Optional[np.ndarray] = None) -> None: \"\"\" Bayesian linear regression model using numpyro. Parameters: - x: np.ndarray, predictor variable(s). - y: Optional[np.ndarray], response variable. If None, the model performs prediction. \"\"\" slope = numpyro.sample('slope', dist.Normal(0, 10)) intercept = numpyro.sample('intercept', dist.Normal(0, 10)) sigma = numpyro.sample('sigma', dist.Exponential(1.0)) mean = slope * x + intercept with numpyro.plate('data', x.shape[0]): numpyro.sample('obs', dist.Normal(mean, sigma), obs=y) def main(): # Set up the NUTS kernel and run MCMC nuts_kernel = NUTS(linear_regression_model) mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000) mcmc.run(jax.random.PRNGKey(0), x=x, y=y) # Get the samples and convert to ArviZ InferenceData object samples = mcmc.get_samples() idata = az.from_numpyro(mcmc) # Plot the posterior distributions and save the plot az.plot_posterior(idata, var_names=['slope', 'intercept', 'sigma']) plt.savefig('plots/linear_regression_posterior_plot.png') # Summary of the posterior distribution print(az.summary(idata, var_names=['slope', 'intercept', 'sigma'])) if __name__ == \"__main__\": main() Example 3 - Lorenz System Lorenz System Prediction Plot matplotlib.use(\"Agg\") # noqa: E402 @jit def dz_dt(z, t, theta): \"\"\" Computes the derivative of the Lorenz system at a point. Parameters: - z: jnp.ndarray, current state of the system [x, y, z]. - t: float, current time (unused as Lorenz equations are autonomous). - theta: jnp.ndarray, parameters of the system [sigma, rho, beta]. Returns: - jnp.ndarray, derivative [dx/dt, dy/dt, dz/dt]. \"\"\" x, y, z = z sigma, rho, beta = theta dx_dt = sigma * (y - x) dy_dt = x * (rho - z) - y dz_dt = x * y - beta * z return jnp.array([dx_dt, dy_dt, dz_dt]) def generate_synthetic_data(theta, z_init, N): \"\"\" Generates synthetic data using the Lorenz model. Parameters: - theta: jnp.ndarray, parameters for the Lorenz model. - z_init: jnp.ndarray, initial state of the system. - N: int, number of data points to generate. Returns: - jnp.ndarray, generated data points. \"\"\" ts = jnp.linspace(0.0, N-1, N) z_init = jnp.array(z_init, dtype=jnp.float32) theta = jnp.array(theta, dtype=jnp.float32) z = odeint(dz_dt, z_init, ts, theta, rtol=1e-6, atol=1e-5, mxstep=1000) return z def model(N, y=None): \"\"\" Bayesian model for inference using the Lorenz system. Parameters: - N: int, number of data points. - y: jnp.ndarray, observed data points. \"\"\" z_init = numpyro.sample(\"z_init\", dist.LogNormal(jnp.log(10), 1).expand([3])) ts = jnp.linspace(0.0, N-1, N) theta = numpyro.sample( \"theta\", dist.TruncatedNormal( low=0.0, loc=jnp.array([10.0, 28.0, 8.0 / 3.0]), scale=jnp.array([0.5, 0.5, 0.5]), ), ) z = odeint(dz_dt, z_init, ts, theta, rtol=1e-6, atol=1e-5, mxstep=1000) sigma = numpyro.sample(\"sigma\", dist.Exponential(1).expand([3])) numpyro.sample(\"y\", dist.Normal(z, sigma), obs=y) def main(args): \"\"\" Main function to perform Bayesian inference and plot results. Parameters: - args: argparse.Namespace, command line arguments. \"\"\" # Parameters for synthetic data true_theta = jnp.array([10.0, 28.0, 8.0 / 3.0]) z_init = jnp.array([1.0, 1.0, 1.0]) N = 30 # Generate synthetic data synthetic_data = generate_synthetic_data(true_theta, z_init, N) # Bayesian inference mcmc = MCMC( NUTS(model, dense_mass=True), num_warmup=args.num_warmup, num_samples=args.num_samples, num_chains=args.num_chains, progress_bar=True, ) mcmc.run(PRNGKey(1), N=N, y=synthetic_data) mcmc.print_summary() # Prediction pred = Predictive(model, mcmc.get_samples())(PRNGKey(2), N)[\"y\"] mu = jnp.mean(pred, 0) pi = jnp.percentile(pred, jnp.array([10, 90]), 0) # Plotting sns.set_theme(style=\"whitegrid\") fig = plt.figure(figsize=(8, 6), constrained_layout=True) ax = fig.add_subplot(111, projection='3d') ax.plot(synthetic_data[:, 0], synthetic_data[:, 1], synthetic_data[:, 2], \"r-\", label=\"synthetic data\") ax.plot(mu[:, 0], mu[:, 1], mu[:, 2], \"b--\", label=\"predicted\") ax.set_xlabel(\"X Axis\") ax.set_ylabel(\"Y Axis\") ax.set_zlabel(\"Z Axis\") plt.title(\"Lorenz Attractor: Synthetic Data and Predicted Trajectory\") plt.legend() plt.savefig(\"plots/lorenz_plot.pdf\") if __name__ == \"__main__\": assert numpyro.__version__.startswith(\"0.13.2\") parser = argparse.ArgumentParser(description=\"Lorenz Model\") parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=500, type=int) parser.add_argument(\"--num-warmup\", nargs=\"?\", default=100, type=int) parser.add_argument(\"--num-chains\", nargs=\"?\", default=1, type=int) parser.add_argument(\"--device\", default=\"cpu\", type=str, help='use \"cpu\" or \"gpu\".') args = parser.parse_args() numpyro.set_platform(args.device) numpyro.set_host_device_count(args.num_chains) main(args)","title":"Probabilistic Programming Examples"},{"location":"prob_prog_ex/#probabilistic-programming-examples","text":"View Project on GitHub","title":"Probabilistic Programming Examples"},{"location":"prob_prog_ex/#project-overview","text":"Created several introductory examples using NumPyro including a model of the lorenz system.","title":"Project Overview"},{"location":"prob_prog_ex/#technologies-used","text":"Python Jax NumPyro Matplotlib","title":"Technologies Used"},{"location":"prob_prog_ex/#imports","text":"import argparse import jax import jax.numpy as jnp from jax import random, jit from jax.experimental.ode import odeint import matplotlib import matplotlib.pyplot as plt import seaborn as sns import numpy as np import numpyro import numpyro.distributions as dist from numpyro.infer import MCMC, NUTS, Predictive import arviz as az import os from typing import Optional, Sequence","title":"Imports"},{"location":"prob_prog_ex/#example-1-coinflip","text":"Coinflip Posterior Plot def coin_flip_model(observed_data: jnp.ndarray) -> None: \"\"\" Bayesian model for coin flipping using a Bernoulli distribution. Parameters: - observed_data: jnp.ndarray, array of observed coin flips (0s and 1s). \"\"\" prob_heads = numpyro.sample(\"prob_heads\", dist.Uniform(0, 1)) numpyro.sample(\"obs\", dist.Bernoulli(prob_heads), obs=observed_data) def run_mcmc(model: callable, data: jnp.ndarray, num_warmup: int = 1000, num_samples: int = 1000, num_chains: int = 1) -> MCMC: \"\"\" Runs MCMC for a given Bayesian model and observed data. Parameters: - model: Callable, the Bayesian model function. - data: jnp.ndarray, the observed data to fit the model to. - num_warmup: int, number of warmup steps. - num_samples: int, number of samples to draw. - num_chains: int, number of MCMC chains to run. Returns: - MCMC object after running the MCMC algorithm. \"\"\" nuts_kernel = NUTS(model) mcmc = MCMC(nuts_kernel, num_warmup=num_warmup, num_samples=num_samples, num_chains=num_chains) mcmc.run(jax.random.PRNGKey(0), observed_data=data) return mcmc def main(): # Generate observed data for a biased coin key = random.PRNGKey(0) prob_heads = 0.7 num_flips = 100 observed_data = (random.uniform(key, (num_flips,)) < prob_heads).astype(int) # Run MCMC mcmc = run_mcmc(coin_flip_model, observed_data) # Convert to ArviZ InferenceData idata = az.from_numpyro(mcmc) # Plotting the posterior ax = az.plot_posterior(idata, var_names=[\"prob_heads\"]) # Save the figure plt.savefig(\"coin_flip_posterior_plot.pdf\") if __name__ == \"__main__\": main()","title":"Example 1 - Coinflip"},{"location":"prob_prog_ex/#example-2-linear-regression","text":"Linear Regression Posterior Plot # True parameters for generating synthetic data true_slope: float = 2.5 true_intercept: float = -1.0 noise_scale: float = 1.0 # Generate synthetic data np.random.seed(0) x: np.ndarray = np.linspace(0, 1, 100) noise: np.ndarray = np.random.normal(scale=noise_scale, size=x.shape) y: np.ndarray = true_slope * x + true_intercept + noise def linear_regression_model(x: np.ndarray, y: Optional[np.ndarray] = None) -> None: \"\"\" Bayesian linear regression model using numpyro. Parameters: - x: np.ndarray, predictor variable(s). - y: Optional[np.ndarray], response variable. If None, the model performs prediction. \"\"\" slope = numpyro.sample('slope', dist.Normal(0, 10)) intercept = numpyro.sample('intercept', dist.Normal(0, 10)) sigma = numpyro.sample('sigma', dist.Exponential(1.0)) mean = slope * x + intercept with numpyro.plate('data', x.shape[0]): numpyro.sample('obs', dist.Normal(mean, sigma), obs=y) def main(): # Set up the NUTS kernel and run MCMC nuts_kernel = NUTS(linear_regression_model) mcmc = MCMC(nuts_kernel, num_warmup=500, num_samples=1000) mcmc.run(jax.random.PRNGKey(0), x=x, y=y) # Get the samples and convert to ArviZ InferenceData object samples = mcmc.get_samples() idata = az.from_numpyro(mcmc) # Plot the posterior distributions and save the plot az.plot_posterior(idata, var_names=['slope', 'intercept', 'sigma']) plt.savefig('plots/linear_regression_posterior_plot.png') # Summary of the posterior distribution print(az.summary(idata, var_names=['slope', 'intercept', 'sigma'])) if __name__ == \"__main__\": main()","title":"Example 2 - Linear Regression"},{"location":"prob_prog_ex/#example-3-lorenz-system","text":"Lorenz System Prediction Plot matplotlib.use(\"Agg\") # noqa: E402 @jit def dz_dt(z, t, theta): \"\"\" Computes the derivative of the Lorenz system at a point. Parameters: - z: jnp.ndarray, current state of the system [x, y, z]. - t: float, current time (unused as Lorenz equations are autonomous). - theta: jnp.ndarray, parameters of the system [sigma, rho, beta]. Returns: - jnp.ndarray, derivative [dx/dt, dy/dt, dz/dt]. \"\"\" x, y, z = z sigma, rho, beta = theta dx_dt = sigma * (y - x) dy_dt = x * (rho - z) - y dz_dt = x * y - beta * z return jnp.array([dx_dt, dy_dt, dz_dt]) def generate_synthetic_data(theta, z_init, N): \"\"\" Generates synthetic data using the Lorenz model. Parameters: - theta: jnp.ndarray, parameters for the Lorenz model. - z_init: jnp.ndarray, initial state of the system. - N: int, number of data points to generate. Returns: - jnp.ndarray, generated data points. \"\"\" ts = jnp.linspace(0.0, N-1, N) z_init = jnp.array(z_init, dtype=jnp.float32) theta = jnp.array(theta, dtype=jnp.float32) z = odeint(dz_dt, z_init, ts, theta, rtol=1e-6, atol=1e-5, mxstep=1000) return z def model(N, y=None): \"\"\" Bayesian model for inference using the Lorenz system. Parameters: - N: int, number of data points. - y: jnp.ndarray, observed data points. \"\"\" z_init = numpyro.sample(\"z_init\", dist.LogNormal(jnp.log(10), 1).expand([3])) ts = jnp.linspace(0.0, N-1, N) theta = numpyro.sample( \"theta\", dist.TruncatedNormal( low=0.0, loc=jnp.array([10.0, 28.0, 8.0 / 3.0]), scale=jnp.array([0.5, 0.5, 0.5]), ), ) z = odeint(dz_dt, z_init, ts, theta, rtol=1e-6, atol=1e-5, mxstep=1000) sigma = numpyro.sample(\"sigma\", dist.Exponential(1).expand([3])) numpyro.sample(\"y\", dist.Normal(z, sigma), obs=y) def main(args): \"\"\" Main function to perform Bayesian inference and plot results. Parameters: - args: argparse.Namespace, command line arguments. \"\"\" # Parameters for synthetic data true_theta = jnp.array([10.0, 28.0, 8.0 / 3.0]) z_init = jnp.array([1.0, 1.0, 1.0]) N = 30 # Generate synthetic data synthetic_data = generate_synthetic_data(true_theta, z_init, N) # Bayesian inference mcmc = MCMC( NUTS(model, dense_mass=True), num_warmup=args.num_warmup, num_samples=args.num_samples, num_chains=args.num_chains, progress_bar=True, ) mcmc.run(PRNGKey(1), N=N, y=synthetic_data) mcmc.print_summary() # Prediction pred = Predictive(model, mcmc.get_samples())(PRNGKey(2), N)[\"y\"] mu = jnp.mean(pred, 0) pi = jnp.percentile(pred, jnp.array([10, 90]), 0) # Plotting sns.set_theme(style=\"whitegrid\") fig = plt.figure(figsize=(8, 6), constrained_layout=True) ax = fig.add_subplot(111, projection='3d') ax.plot(synthetic_data[:, 0], synthetic_data[:, 1], synthetic_data[:, 2], \"r-\", label=\"synthetic data\") ax.plot(mu[:, 0], mu[:, 1], mu[:, 2], \"b--\", label=\"predicted\") ax.set_xlabel(\"X Axis\") ax.set_ylabel(\"Y Axis\") ax.set_zlabel(\"Z Axis\") plt.title(\"Lorenz Attractor: Synthetic Data and Predicted Trajectory\") plt.legend() plt.savefig(\"plots/lorenz_plot.pdf\") if __name__ == \"__main__\": assert numpyro.__version__.startswith(\"0.13.2\") parser = argparse.ArgumentParser(description=\"Lorenz Model\") parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=500, type=int) parser.add_argument(\"--num-warmup\", nargs=\"?\", default=100, type=int) parser.add_argument(\"--num-chains\", nargs=\"?\", default=1, type=int) parser.add_argument(\"--device\", default=\"cpu\", type=str, help='use \"cpu\" or \"gpu\".') args = parser.parse_args() numpyro.set_platform(args.device) numpyro.set_host_device_count(args.num_chains) main(args)","title":"Example 3 - Lorenz System"},{"location":"prob_prog_tools/","text":"Probabilistic Programming Tools View Project on GitHub Project Overview Designed tools for probabalistic programming with the following purposes: Run a Hamiltonian Monte Carlo Sampler in native JAX Simulate a random levy process and then uses NumPyro to infer the parameters used to simulate those processes Technologies Used Python NumPy Jax NumPyro HMC Imports import jax import jax.numpy as jnp from jax import random, grad,jit import matplotlib.pyplot as plt from jax.scipy.stats import beta, bernoulli from functools import partial HMC Functions def log_posterior( p: float, alpha: float, beta_param: float, heads: int, tails: int ) -> float: \"\"\"Calculates the log-posterior for Bayesian coin flip model. Args: p: Probability of heads (between 0 and 1). alpha: Alpha parameter of beta prior distribution. beta_param: Beta parameter of beta prior distribution. heads: Number of observed heads. tails: Number of observed tails. Returns: The log-posterior value. \"\"\" def potential_fn(params: dict) -> float: \"\"\"Calculates the negative log-posterior (potential function). Args: params: A dictionary containing the 'p' parameter. Returns: The negative log-posterior value. \"\"\" def grad_potential_fn(params: dict) -> float: \"\"\"Calculates the gradient of the negative log-posterior. Args: params: A dictionary containing the 'p' parameter. Returns: The gradient of the negative log-posterior. \"\"\" def leapfrog( grad_log_prob: callable, position: float, momentum: float, step_size: float, num_steps: int, alpha: float, beta_param: float, heads: int, tails: int ) -> tuple: \"\"\"Implements the leapfrog integrator for Hamiltonian dynamics. Args: grad_log_prob: A callable function to calculate the gradient of the log probability. position: Current position (parameter value). momentum: Current momentum. step_size: Step size for the integrator. num_steps: Number of leapfrog steps to take. alpha: Alpha parameter of the beta prior. beta_param: Beta parameter of the beta prior. heads: Number of observed heads. tails: Number of observed tails. Returns: A tuple containing the new position and new momentum. \"\"\" def hmc_step( rng_key: jnp.ndarray, position: float, step_size: float, num_leapfrog_steps: int, alpha: float, beta_param: float, heads: int, tails: int ) -> tuple: \"\"\"Performs a single Hamiltonian Monte Carlo (HMC) step. Args: rng_key: JAX PRNGKey for random number generation. position: Current position (parameter value). step_size: Step size for the leapfrog integrator. num_leapfrog_steps: Number of leapfrog steps to take. alpha: Alpha parameter of the beta prior. beta_param: Beta parameter of the beta prior. heads: Number of observed heads. tails: Number of observed tails. Returns: A tuple containing the new position and whether the step was accepted. \"\"\" def get_hmc_config() -> dict: \"\"\"Returns a dictionary containing the HMC configuration parameters.\"\"\" Example Usage def get_hmc_config() -> dict: \"\"\"Returns a dictionary containing the HMC configuration parameters.\"\"\" return { \"alpha\": 1, # Alpha parameter for beta prior \"beta_prior\": 1, # Beta parameter for beta prior \"heads\": 40, # Number of observed heads \"tails\": 20, # Number of observed tails \"num_burnin\": 500, # Number of burn-in samples \"num_samples\": 5500, # Total number of samples (including burn-in) \"step_size\": 0.05, # HMC step size \"num_leapfrog_steps\": 30 # Number of leapfrog steps } def main(): \"\"\"Executes the HMC sampling procedure.\"\"\" config = get_hmc_config() rng_key = random.PRNGKey(0) samples = [] position = 0.5 # Initial position for _ in range(config[\"num_samples\"]): rng_key, subkey = random.split(rng_key) position, _ = hmc_step( subkey, position, config[\"step_size\"], config[\"num_leapfrog_steps\"], config[\"alpha\"], config[\"beta_prior\"], config[\"heads\"], config[\"tails\"] ) samples.append(position) samples = samples[config[\"num_burnin\"]:] print(f\"Estimated probability of heads: {jnp.mean(jnp.array(samples))}\") if __name__ == \"__main__\": main() Random Process Imports from scipy.stats import levy_stable from jax.scipy.linalg import cholesky from typing import Dict, List, Tuple import os from typing import Any, Dict import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpyro import numpyro.distributions as dist import seaborn as sns from numpyro.infer import MCMC, NUTS Classes StockMarketSimulator class StockMarketSimulator: \"\"\" A simulator for stock market prices using Levy processes with JAX for computation and Pareto-distributed initial prices using NumPy. Attributes: n_industries (int): Number of industries. n_stocks_per_industry (int): Number of stocks per industry. base_stock_price (float): Base stock price for scaling initial prices. industries (List[str]): List of industry names. stocks (List[str]): List of stock symbols. stock_prices (pd.DataFrame): DataFrame to store simulated stock prices. seed (int): Int derived through os for true random state key (jax.random.PRNGKey): JAX PRNG key for random number generation. industry_map (Dict[str, str]): Mapping of stocks to their respective industries. alpha_params (Dict[str, float]): Alpha parameter for each industry. beta_params (Dict[str, float]): Beta parameter for each industry. pareto_shapes (Dict[str, float]): Pareto shape parameter for each industry. \"\"\" Methods init () def __init__(self, n_industries: int = 8, n_stocks_per_industry: int = 10, base_stock_price: float = 100) -> None: \"\"\" Initialize the stock market simulator with specified parameters. Args: n_industries (int): The number of industries to simulate. n_stocks_per_industry (int): The number of stocks per industry. base_stock_price (float): The base stock price for scaling initial prices. \"\"\" _initialize_prices() def _initialize_prices(self) -> None: \"\"\"Initialize the stock prices using Pareto distribution for each industry.\"\"\" _simulate_stock_prices() def simulate_stock_prices(self, n_days: int = 252) -> pd.DataFrame: \"\"\" Simulate stock prices over a given number of days. Args: n_days (int): The number of days to simulate stock prices. Returns: pd.DataFrame: A DataFrame containing the simulated stock prices. \"\"\" _apply_correlation def _apply_correlation(self, increments: np.ndarray) -> np.ndarray: \"\"\" Apply a correlation matrix to the increments using Cholesky decomposition. Args: increments (np.ndarray): An array of increments to apply correlation to. Returns: np.ndarray: Correlated increments after applying the correlation matrix. \"\"\" Functions plot_posteriors() def plot_posteriors(posterior_samples: Dict[str, jnp.ndarray], industry: str) -> None: \"\"\" Plots the posterior distributions for a given industry. Args: posterior_samples: Samples from the posterior distribution as a dictionary where keys are parameter names. industry (str): The name of the industry for which the posterior distributions are plotted. \"\"\" run_bayesian_inference() def run_bayesian_inference(simulator: DI.StockMarketSimulator, n_samples: int = 500, n_warmup: int = 100) -> None: \"\"\" Runs Bayesian inference for each industry and plots the posterior distributions. Args: simulator: An instance of StockMarketSimulator containing stock prices and industry mappings. n_samples (int): Number of samples to draw from the posterior distribution. n_warmup (int): Number of warmup steps for the sampler. \"\"\" plot_stock_prices() def plot_stock_prices(simulator: DI.StockMarketSimulator) -> None: \"\"\" Plot the simulated stock prices for all stocks in the simulation. Args: simulator (DI.StockMarketSimulator): An instance of the StockMarketSimulator class. This function generates a line plot for each stock across the simulated days and saves the plot to a file named 'simulated_stock_prices.png' in the 'plots' directory. \"\"\" plot_stock_prices_by_industry() def plot_stock_prices_by_industry(simulator: DI.StockMarketSimulator) -> None: \"\"\" Plot the simulated stock prices for each industry separately. Args: simulator (DI.StockMarketSimulator): An instance of the StockMarketSimulator class. This function generates a line plot for each stock within an industry across the simulated days and saves each industry's plot to a separate file in the 'plots' directory, named 'simulated_stock_price_by_[industry].png'. \"\"\" Example Usage def main() -> None: \"\"\" Main function to initialize the simulator, run the stock price simulation, and plot the results. \"\"\" simulator = DI.StockMarketSimulator() run_bayesian_inference(simulator) simulator.simulate_stock_prices() plot_stock_prices(simulator) plot_stock_prices_by_industry(simulator) if __name__ == \"__main__\": main() Experience | Education | Skills | Projects TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Probabilistic Programming Tools"},{"location":"prob_prog_tools/#probabilistic-programming-tools","text":"View Project on GitHub","title":"Probabilistic Programming Tools"},{"location":"prob_prog_tools/#project-overview","text":"Designed tools for probabalistic programming with the following purposes: Run a Hamiltonian Monte Carlo Sampler in native JAX Simulate a random levy process and then uses NumPyro to infer the parameters used to simulate those processes","title":"Project Overview"},{"location":"prob_prog_tools/#technologies-used","text":"Python NumPy Jax NumPyro","title":"Technologies Used"},{"location":"prob_prog_tools/#hmc-imports","text":"import jax import jax.numpy as jnp from jax import random, grad,jit import matplotlib.pyplot as plt from jax.scipy.stats import beta, bernoulli from functools import partial","title":"HMC Imports"},{"location":"prob_prog_tools/#hmc-functions","text":"def log_posterior( p: float, alpha: float, beta_param: float, heads: int, tails: int ) -> float: \"\"\"Calculates the log-posterior for Bayesian coin flip model. Args: p: Probability of heads (between 0 and 1). alpha: Alpha parameter of beta prior distribution. beta_param: Beta parameter of beta prior distribution. heads: Number of observed heads. tails: Number of observed tails. Returns: The log-posterior value. \"\"\" def potential_fn(params: dict) -> float: \"\"\"Calculates the negative log-posterior (potential function). Args: params: A dictionary containing the 'p' parameter. Returns: The negative log-posterior value. \"\"\" def grad_potential_fn(params: dict) -> float: \"\"\"Calculates the gradient of the negative log-posterior. Args: params: A dictionary containing the 'p' parameter. Returns: The gradient of the negative log-posterior. \"\"\" def leapfrog( grad_log_prob: callable, position: float, momentum: float, step_size: float, num_steps: int, alpha: float, beta_param: float, heads: int, tails: int ) -> tuple: \"\"\"Implements the leapfrog integrator for Hamiltonian dynamics. Args: grad_log_prob: A callable function to calculate the gradient of the log probability. position: Current position (parameter value). momentum: Current momentum. step_size: Step size for the integrator. num_steps: Number of leapfrog steps to take. alpha: Alpha parameter of the beta prior. beta_param: Beta parameter of the beta prior. heads: Number of observed heads. tails: Number of observed tails. Returns: A tuple containing the new position and new momentum. \"\"\" def hmc_step( rng_key: jnp.ndarray, position: float, step_size: float, num_leapfrog_steps: int, alpha: float, beta_param: float, heads: int, tails: int ) -> tuple: \"\"\"Performs a single Hamiltonian Monte Carlo (HMC) step. Args: rng_key: JAX PRNGKey for random number generation. position: Current position (parameter value). step_size: Step size for the leapfrog integrator. num_leapfrog_steps: Number of leapfrog steps to take. alpha: Alpha parameter of the beta prior. beta_param: Beta parameter of the beta prior. heads: Number of observed heads. tails: Number of observed tails. Returns: A tuple containing the new position and whether the step was accepted. \"\"\" def get_hmc_config() -> dict: \"\"\"Returns a dictionary containing the HMC configuration parameters.\"\"\"","title":"HMC Functions"},{"location":"prob_prog_tools/#example-usage","text":"def get_hmc_config() -> dict: \"\"\"Returns a dictionary containing the HMC configuration parameters.\"\"\" return { \"alpha\": 1, # Alpha parameter for beta prior \"beta_prior\": 1, # Beta parameter for beta prior \"heads\": 40, # Number of observed heads \"tails\": 20, # Number of observed tails \"num_burnin\": 500, # Number of burn-in samples \"num_samples\": 5500, # Total number of samples (including burn-in) \"step_size\": 0.05, # HMC step size \"num_leapfrog_steps\": 30 # Number of leapfrog steps } def main(): \"\"\"Executes the HMC sampling procedure.\"\"\" config = get_hmc_config() rng_key = random.PRNGKey(0) samples = [] position = 0.5 # Initial position for _ in range(config[\"num_samples\"]): rng_key, subkey = random.split(rng_key) position, _ = hmc_step( subkey, position, config[\"step_size\"], config[\"num_leapfrog_steps\"], config[\"alpha\"], config[\"beta_prior\"], config[\"heads\"], config[\"tails\"] ) samples.append(position) samples = samples[config[\"num_burnin\"]:] print(f\"Estimated probability of heads: {jnp.mean(jnp.array(samples))}\") if __name__ == \"__main__\": main()","title":"Example Usage"},{"location":"prob_prog_tools/#random-process-imports","text":"from scipy.stats import levy_stable from jax.scipy.linalg import cholesky from typing import Dict, List, Tuple import os from typing import Any, Dict import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpyro import numpyro.distributions as dist import seaborn as sns from numpyro.infer import MCMC, NUTS","title":"Random Process Imports"},{"location":"prob_prog_tools/#classes","text":"","title":"Classes"},{"location":"prob_prog_tools/#stockmarketsimulator","text":"class StockMarketSimulator: \"\"\" A simulator for stock market prices using Levy processes with JAX for computation and Pareto-distributed initial prices using NumPy. Attributes: n_industries (int): Number of industries. n_stocks_per_industry (int): Number of stocks per industry. base_stock_price (float): Base stock price for scaling initial prices. industries (List[str]): List of industry names. stocks (List[str]): List of stock symbols. stock_prices (pd.DataFrame): DataFrame to store simulated stock prices. seed (int): Int derived through os for true random state key (jax.random.PRNGKey): JAX PRNG key for random number generation. industry_map (Dict[str, str]): Mapping of stocks to their respective industries. alpha_params (Dict[str, float]): Alpha parameter for each industry. beta_params (Dict[str, float]): Beta parameter for each industry. pareto_shapes (Dict[str, float]): Pareto shape parameter for each industry. \"\"\"","title":"StockMarketSimulator"},{"location":"prob_prog_tools/#methods","text":"","title":"Methods"},{"location":"prob_prog_tools/#init","text":"def __init__(self, n_industries: int = 8, n_stocks_per_industry: int = 10, base_stock_price: float = 100) -> None: \"\"\" Initialize the stock market simulator with specified parameters. Args: n_industries (int): The number of industries to simulate. n_stocks_per_industry (int): The number of stocks per industry. base_stock_price (float): The base stock price for scaling initial prices. \"\"\"","title":"init()"},{"location":"prob_prog_tools/#_initialize_prices","text":"def _initialize_prices(self) -> None: \"\"\"Initialize the stock prices using Pareto distribution for each industry.\"\"\"","title":"_initialize_prices()"},{"location":"prob_prog_tools/#_simulate_stock_prices","text":"def simulate_stock_prices(self, n_days: int = 252) -> pd.DataFrame: \"\"\" Simulate stock prices over a given number of days. Args: n_days (int): The number of days to simulate stock prices. Returns: pd.DataFrame: A DataFrame containing the simulated stock prices. \"\"\"","title":"_simulate_stock_prices()"},{"location":"prob_prog_tools/#_apply_correlation","text":"def _apply_correlation(self, increments: np.ndarray) -> np.ndarray: \"\"\" Apply a correlation matrix to the increments using Cholesky decomposition. Args: increments (np.ndarray): An array of increments to apply correlation to. Returns: np.ndarray: Correlated increments after applying the correlation matrix. \"\"\"","title":"_apply_correlation"},{"location":"prob_prog_tools/#functions","text":"","title":"Functions"},{"location":"prob_prog_tools/#plot_posteriors","text":"def plot_posteriors(posterior_samples: Dict[str, jnp.ndarray], industry: str) -> None: \"\"\" Plots the posterior distributions for a given industry. Args: posterior_samples: Samples from the posterior distribution as a dictionary where keys are parameter names. industry (str): The name of the industry for which the posterior distributions are plotted. \"\"\"","title":"plot_posteriors()"},{"location":"prob_prog_tools/#run_bayesian_inference","text":"def run_bayesian_inference(simulator: DI.StockMarketSimulator, n_samples: int = 500, n_warmup: int = 100) -> None: \"\"\" Runs Bayesian inference for each industry and plots the posterior distributions. Args: simulator: An instance of StockMarketSimulator containing stock prices and industry mappings. n_samples (int): Number of samples to draw from the posterior distribution. n_warmup (int): Number of warmup steps for the sampler. \"\"\"","title":"run_bayesian_inference()"},{"location":"prob_prog_tools/#plot_stock_prices","text":"def plot_stock_prices(simulator: DI.StockMarketSimulator) -> None: \"\"\" Plot the simulated stock prices for all stocks in the simulation. Args: simulator (DI.StockMarketSimulator): An instance of the StockMarketSimulator class. This function generates a line plot for each stock across the simulated days and saves the plot to a file named 'simulated_stock_prices.png' in the 'plots' directory. \"\"\"","title":"plot_stock_prices()"},{"location":"prob_prog_tools/#plot_stock_prices_by_industry","text":"def plot_stock_prices_by_industry(simulator: DI.StockMarketSimulator) -> None: \"\"\" Plot the simulated stock prices for each industry separately. Args: simulator (DI.StockMarketSimulator): An instance of the StockMarketSimulator class. This function generates a line plot for each stock within an industry across the simulated days and saves each industry's plot to a separate file in the 'plots' directory, named 'simulated_stock_price_by_[industry].png'. \"\"\"","title":"plot_stock_prices_by_industry()"},{"location":"prob_prog_tools/#example-usage_1","text":"def main() -> None: \"\"\" Main function to initialize the simulator, run the stock price simulation, and plot the results. \"\"\" simulator = DI.StockMarketSimulator() run_bayesian_inference(simulator) simulator.simulate_stock_prices() plot_stock_prices(simulator) plot_stock_prices_by_industry(simulator) if __name__ == \"__main__\": main() Experience | Education | Skills | Projects TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Example Usage"},{"location":"projects/","text":"Projects TI4 Combat Simulator Technologies Used : Python, Jax, Matplotlib Developed a combat simulator for the board game \"Twilight Imperium 4th Edition\" (TI4), utilizing Monte Carlo and probability theory to model the distribution of outcomes for in-game combat scenarios Baseball Pitch Predictor Technologies Used : Python, Jax, Flax Created a predictive model to analyze and forecast baseball pitch types based on historical pitch data Probabilistic Programming Tools Technologies Used : Python, NumPy, Jax, NumPyro Designed a stock market simulation tool that simulates a random levy process and then uses NumPyro infers the parameters used to simulate those processes Probabilistic Programming Examples Technologies Used : Python, Jax, NumPyro, Matplotlib Created several introductory examples using NumPyro including a model of the lorenz system. Data Tools for Economic Research Technologies Used : Python, SQL Developed a suite of data analysis tools tailored for economic research and the cleaning and ingestion of economic and financial API data into a SQLite database Nucleation of Asset Bubbles Technologies Used : Bloomberg, Python, Excel, R, LaTeX This project explored the relationship between the physical nucleation of bubbles and the financial formation of asset bubbles The Chaotic Nature of the Complex Exponential Map Technologies Used : LaTeX Created an accessible introduction to the chaotic properties of the exponential map on the complex plane MCMC with Efficient Jumps Technologies Used : LaTeX Analyzed a research paper that uses an efficient jump MCMC method to model interest-rate dynamics Experience | Education | Skills | Projects","title":"Projects"},{"location":"projects/#projects","text":"","title":"Projects"},{"location":"projects/#ti4-combat-simulator","text":"Technologies Used : Python, Jax, Matplotlib Developed a combat simulator for the board game \"Twilight Imperium 4th Edition\" (TI4), utilizing Monte Carlo and probability theory to model the distribution of outcomes for in-game combat scenarios","title":"TI4 Combat Simulator"},{"location":"projects/#baseball-pitch-predictor","text":"Technologies Used : Python, Jax, Flax Created a predictive model to analyze and forecast baseball pitch types based on historical pitch data","title":"Baseball Pitch Predictor"},{"location":"projects/#probabilistic-programming-tools","text":"Technologies Used : Python, NumPy, Jax, NumPyro Designed a stock market simulation tool that simulates a random levy process and then uses NumPyro infers the parameters used to simulate those processes","title":"Probabilistic Programming Tools"},{"location":"projects/#probabilistic-programming-examples","text":"Technologies Used : Python, Jax, NumPyro, Matplotlib Created several introductory examples using NumPyro including a model of the lorenz system.","title":"Probabilistic Programming Examples"},{"location":"projects/#data-tools-for-economic-research","text":"Technologies Used : Python, SQL Developed a suite of data analysis tools tailored for economic research and the cleaning and ingestion of economic and financial API data into a SQLite database","title":"Data Tools for Economic Research"},{"location":"projects/#nucleation-of-asset-bubbles","text":"Technologies Used : Bloomberg, Python, Excel, R, LaTeX This project explored the relationship between the physical nucleation of bubbles and the financial formation of asset bubbles","title":"Nucleation of Asset Bubbles"},{"location":"projects/#the-chaotic-nature-of-the-complex-exponential-map","text":"Technologies Used : LaTeX Created an accessible introduction to the chaotic properties of the exponential map on the complex plane","title":"The Chaotic Nature of the Complex Exponential Map"},{"location":"projects/#mcmc-with-efficient-jumps","text":"Technologies Used : LaTeX Analyzed a research paper that uses an efficient jump MCMC method to model interest-rate dynamics Experience | Education | Skills | Projects","title":"MCMC with Efficient Jumps"},{"location":"skills/","text":"Skills Python : Proficient in Python programming for end-to-end data processing and modeling Pandas NumPy Matplotlib Jax NumPyro R : Experienced in R programming for data cleaning, analysis, and visualization data.table ggplot2 quantmod rstan SQL : Experienced in SQL database management, querying, and data manipulation AWS : Familiarity with AWS services such as Lambda, S3, and Athena Tableau : Skilled in using Tableau to inform decision makers with insightful dashboards that are visually appealing Power BI : Similarly proficient in creating informative dashboards in Power BI Excel : Advanced proficiency in Excel including VLookups, pivot tables, and basic logical functions Passions Data Modeling : Enthusiastic about constructing accurate analytical frameworks to uncover insights and inform strategic decision-making Probabilistic Programming : Fascinated by the use of Markov Chain Monte Carlo (MCMC) methods to explain randomness in data Generative AI : Interested in exploring the capabilities of Large Language Models (LLMs) for building architecture and solving unique problems Experience | Education | Skills | Projects","title":"Skills"},{"location":"skills/#skills","text":"Python : Proficient in Python programming for end-to-end data processing and modeling Pandas NumPy Matplotlib Jax NumPyro R : Experienced in R programming for data cleaning, analysis, and visualization data.table ggplot2 quantmod rstan SQL : Experienced in SQL database management, querying, and data manipulation AWS : Familiarity with AWS services such as Lambda, S3, and Athena Tableau : Skilled in using Tableau to inform decision makers with insightful dashboards that are visually appealing Power BI : Similarly proficient in creating informative dashboards in Power BI Excel : Advanced proficiency in Excel including VLookups, pivot tables, and basic logical functions","title":"Skills"},{"location":"skills/#passions","text":"Data Modeling : Enthusiastic about constructing accurate analytical frameworks to uncover insights and inform strategic decision-making Probabilistic Programming : Fascinated by the use of Markov Chain Monte Carlo (MCMC) methods to explain randomness in data Generative AI : Interested in exploring the capabilities of Large Language Models (LLMs) for building architecture and solving unique problems Experience | Education | Skills | Projects","title":"Passions"},{"location":"ti4_combat_simulator/","text":"TI4 Combat Simulator View Project on GitHub | Output graph Project Background Twilight Imperium (TI4) is a strategy board game known for its intricate diplomacy, expansive empire-building, and epic space battles. Engaging in combat within TI4 demands a sophisticated interplay of analysis, meticulous planning, and an element of chance. The combat mechanics of TI4 infuse unpredictability by integrating dice rolls to influence battle outcomes. Historically, navigating TI4's combat landscape has been a challenge, rendering forecasting methods based solely on unit statistics and strategic maneuvers insufficient. Project Overview This combat simulator for TI4 utilizes Monte Carlo methods and probability theory to model the distribution of outcomes for in-game combat scenarios. This tool helps players understand the potential outcomes of their strategic decisions in combat, aiding in planning and execution during gameplay. Technologies Used Python Jax Seaborn This document describes a set of functions for simulating combat scenarios using JAX. Imports import jax.numpy as jnp from jax import random import os from typing import Dict, Tuple, Any import matplotlib.pyplot as plt from collections import defaultdict import pandas as pd import seaborn as sns Types KeyType = Tuple[int, float] SideType = Dict[KeyType, int] RNGKey = Any Functions apply_hits def apply_hits(side: SideType, hits_scored: int, rng_key: RNGKey) -> SideType: \"\"\" Apply hits to a side with JAX, prioritizing dice with lower hit probabilities and lower health. Incorporates randomness in selecting dice within the same priority level to take hits. Accepts an RNG key for reproducible randomness. Parameters: side (SideType): Dictionary representing the side's units and their stats. hits_scored (int): Number of hits to apply to the side. rng_key (RNGKey): JAX random key for generating random numbers. Returns: SideType: Updated side after applying hits. \"\"\" simulate_combat_round def simulate_combat_round(side_a: SideType, side_b: SideType, rng_key: RNGKey) -> Tuple[SideType, SideType]: \"\"\" Simulates a single round of combat between two sides. Parameters: side_a (SideType): The initial state of side A. side_b (SideType): The initial state of side B. rng_key (RNGKey): A random key for JAX's random number generation. Returns: Tuple[SideType, SideType]: The updated states of side A and side B after the combat round. \"\"\" run_combat_until_elimination def run_combat_until_elimination(side_a: SideType, side_b: SideType, rng_key: RNGKey, max_rounds: int = 1000) -> Tuple[SideType, SideType, int, SideType, SideType]: \"\"\" Runs combat rounds between two sides until one side is eliminated or a maximum number of rounds is reached, also returns the initial states. Parameters: - side_a (SideType): Initial state of side A. - side_b (SideType): Initial state of side B. - rng_key (RNGKey): JAX random key for generating random numbers. - max_rounds (int, optional): Maximum number of rounds to simulate. Defaults to 1000. Returns: - Tuple[SideType, SideType, int, SideType, SideType]: Final states of side A and side B, the number of rounds simulated, and copies of the initial states of side A and side B. \"\"\" state_to_string def state_to_string(state: SideType) -> str: \"\"\" Converts a side's state to a string representation for consistent key usage. This function sorts the units in the side by their hit probabilities and health before converting to string, ensuring consistent string representation for identical states. Parameters: - state (SideType): The current state of the side's units, represented as a dictionary with keys as (unit health, hit probability) and values as unit counts. Returns: - str: A string representation of the state, sorted by unit properties. \"\"\" calculate_health def calculate_health(state_str: str) -> int: \"\"\" Calculates the total health score based on a side's state string representation. This function interprets the state string, extracting unit health and count to compute the overall health score of the side. Parameters: - state_str (str): A string representation of a side's state, typically obtained from `state_to_string` function. Returns: - int: The total health score calculated from the state. \"\"\" monte_carlo_combat_simulation def mc(initial_side_a,initial_side_b,num_simulations): \"\"\" Conducts a Monte Carlo simulation to estimate the outcome probabilities of combat between two sides. Parameters: initial_side_a (SideType): The initial state of side A. initial_side_b (SideType): The initial state of side B. num_simulations (int): The number of simulations to run. Returns: Tuple[List[str], List[int]]: The outcomes (as labels) and their counts. \"\"\" normalize def normalize(all_labels,all_values): \"\"\" Normalizes the simulation counts to probabilities Parameters: all_labels (List[str]): The labels for each outcome. all_values (List[int]): The counts for each outcome. filename (str): The file path to save the plot. Returns: data (pd.DataFrame): data to plot \"\"\" plot_and_simulate_data def plot_and_simulate_data(initial_side_a,initial_side_b,num_simulations): \"\"\" Runs Monte Carlo simulations to estimate combat outcomes between two sides and plots the results. This function orchestrates the simulation process by calling the appropriate functions to simulate combat, calculate outcome probabilities, and plot the probabilities in a bar plot. Parameters: - initial_side_a (SideType): The initial state of side A. - initial_side_b (SideType): The initial state of side B. - num_simulations (int): The number of simulations to run for estimating outcomes. The function does not return a value but generates and saves a plot visualizing the probability of different outcomes based on the simulations. \"\"\" Example Usage The following is an example of 2 dreadnaughts, each with 2 health hitting 60% of the time fighting vs 6 fighters, each with 1 health hitting 20% of the time. initial_side_a = {(2, 0.6): 3} initial_side_b = {(1, 0.2): 6} num_simulations = 10000 plot_and_simulate_data(initial_side_a,initial_side_b,num_simulations) Experience | Education | Skills | Projects TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"TI4 Combat Simulator"},{"location":"ti4_combat_simulator/#ti4-combat-simulator","text":"View Project on GitHub | Output graph","title":"TI4 Combat Simulator"},{"location":"ti4_combat_simulator/#project-background","text":"Twilight Imperium (TI4) is a strategy board game known for its intricate diplomacy, expansive empire-building, and epic space battles. Engaging in combat within TI4 demands a sophisticated interplay of analysis, meticulous planning, and an element of chance. The combat mechanics of TI4 infuse unpredictability by integrating dice rolls to influence battle outcomes. Historically, navigating TI4's combat landscape has been a challenge, rendering forecasting methods based solely on unit statistics and strategic maneuvers insufficient.","title":"Project Background"},{"location":"ti4_combat_simulator/#project-overview","text":"This combat simulator for TI4 utilizes Monte Carlo methods and probability theory to model the distribution of outcomes for in-game combat scenarios. This tool helps players understand the potential outcomes of their strategic decisions in combat, aiding in planning and execution during gameplay.","title":"Project Overview"},{"location":"ti4_combat_simulator/#technologies-used","text":"Python Jax Seaborn This document describes a set of functions for simulating combat scenarios using JAX.","title":"Technologies Used"},{"location":"ti4_combat_simulator/#imports","text":"import jax.numpy as jnp from jax import random import os from typing import Dict, Tuple, Any import matplotlib.pyplot as plt from collections import defaultdict import pandas as pd import seaborn as sns","title":"Imports"},{"location":"ti4_combat_simulator/#types","text":"KeyType = Tuple[int, float] SideType = Dict[KeyType, int] RNGKey = Any","title":"Types"},{"location":"ti4_combat_simulator/#functions","text":"","title":"Functions"},{"location":"ti4_combat_simulator/#apply_hits","text":"def apply_hits(side: SideType, hits_scored: int, rng_key: RNGKey) -> SideType: \"\"\" Apply hits to a side with JAX, prioritizing dice with lower hit probabilities and lower health. Incorporates randomness in selecting dice within the same priority level to take hits. Accepts an RNG key for reproducible randomness. Parameters: side (SideType): Dictionary representing the side's units and their stats. hits_scored (int): Number of hits to apply to the side. rng_key (RNGKey): JAX random key for generating random numbers. Returns: SideType: Updated side after applying hits. \"\"\"","title":"apply_hits"},{"location":"ti4_combat_simulator/#simulate_combat_round","text":"def simulate_combat_round(side_a: SideType, side_b: SideType, rng_key: RNGKey) -> Tuple[SideType, SideType]: \"\"\" Simulates a single round of combat between two sides. Parameters: side_a (SideType): The initial state of side A. side_b (SideType): The initial state of side B. rng_key (RNGKey): A random key for JAX's random number generation. Returns: Tuple[SideType, SideType]: The updated states of side A and side B after the combat round. \"\"\"","title":"simulate_combat_round"},{"location":"ti4_combat_simulator/#run_combat_until_elimination","text":"def run_combat_until_elimination(side_a: SideType, side_b: SideType, rng_key: RNGKey, max_rounds: int = 1000) -> Tuple[SideType, SideType, int, SideType, SideType]: \"\"\" Runs combat rounds between two sides until one side is eliminated or a maximum number of rounds is reached, also returns the initial states. Parameters: - side_a (SideType): Initial state of side A. - side_b (SideType): Initial state of side B. - rng_key (RNGKey): JAX random key for generating random numbers. - max_rounds (int, optional): Maximum number of rounds to simulate. Defaults to 1000. Returns: - Tuple[SideType, SideType, int, SideType, SideType]: Final states of side A and side B, the number of rounds simulated, and copies of the initial states of side A and side B. \"\"\"","title":"run_combat_until_elimination"},{"location":"ti4_combat_simulator/#state_to_string","text":"def state_to_string(state: SideType) -> str: \"\"\" Converts a side's state to a string representation for consistent key usage. This function sorts the units in the side by their hit probabilities and health before converting to string, ensuring consistent string representation for identical states. Parameters: - state (SideType): The current state of the side's units, represented as a dictionary with keys as (unit health, hit probability) and values as unit counts. Returns: - str: A string representation of the state, sorted by unit properties. \"\"\"","title":"state_to_string"},{"location":"ti4_combat_simulator/#calculate_health","text":"def calculate_health(state_str: str) -> int: \"\"\" Calculates the total health score based on a side's state string representation. This function interprets the state string, extracting unit health and count to compute the overall health score of the side. Parameters: - state_str (str): A string representation of a side's state, typically obtained from `state_to_string` function. Returns: - int: The total health score calculated from the state. \"\"\"","title":"calculate_health"},{"location":"ti4_combat_simulator/#monte_carlo_combat_simulation","text":"def mc(initial_side_a,initial_side_b,num_simulations): \"\"\" Conducts a Monte Carlo simulation to estimate the outcome probabilities of combat between two sides. Parameters: initial_side_a (SideType): The initial state of side A. initial_side_b (SideType): The initial state of side B. num_simulations (int): The number of simulations to run. Returns: Tuple[List[str], List[int]]: The outcomes (as labels) and their counts. \"\"\"","title":"monte_carlo_combat_simulation"},{"location":"ti4_combat_simulator/#normalize","text":"def normalize(all_labels,all_values): \"\"\" Normalizes the simulation counts to probabilities Parameters: all_labels (List[str]): The labels for each outcome. all_values (List[int]): The counts for each outcome. filename (str): The file path to save the plot. Returns: data (pd.DataFrame): data to plot \"\"\"","title":"normalize"},{"location":"ti4_combat_simulator/#plot_and_simulate_data","text":"def plot_and_simulate_data(initial_side_a,initial_side_b,num_simulations): \"\"\" Runs Monte Carlo simulations to estimate combat outcomes between two sides and plots the results. This function orchestrates the simulation process by calling the appropriate functions to simulate combat, calculate outcome probabilities, and plot the probabilities in a bar plot. Parameters: - initial_side_a (SideType): The initial state of side A. - initial_side_b (SideType): The initial state of side B. - num_simulations (int): The number of simulations to run for estimating outcomes. The function does not return a value but generates and saves a plot visualizing the probability of different outcomes based on the simulations. \"\"\"","title":"plot_and_simulate_data"},{"location":"ti4_combat_simulator/#example-usage","text":"The following is an example of 2 dreadnaughts, each with 2 health hitting 60% of the time fighting vs 6 fighters, each with 1 health hitting 20% of the time. initial_side_a = {(2, 0.6): 3} initial_side_b = {(1, 0.2): 6} num_simulations = 10000 plot_and_simulate_data(initial_side_a,initial_side_b,num_simulations) Experience | Education | Skills | Projects TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Example Usage"}]}