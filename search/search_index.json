{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Jacob Rider Professional Summary I am a math and data enthusiast looking for a position where I can build probabilistic models of complex data in order to inform decision making (with a little bit of help from generative AI). In my most recent jobs, I learned the extract, transform, and load (ETL) process using a variety of methods. I also have experience creating dashboards and KPIs using Tableau and Excel. I believe these experiences uniquely position me to own the data modeling process from end-to-end. Skills: Python, SQL, AWS, Tableau, Excel Passions: Generative AI, probabilistic programming, and data modeling Soft Skills: Concise, empathetic and courteous communication Hobbies: Board games, travel, and food jacob.rider34@gmail.com | LinkedIn | GitHub | Website Experience | Education | Skills | Projects | Contact","title":"Home"},{"location":"#jacob-rider","text":"","title":"Jacob Rider"},{"location":"#professional-summary","text":"I am a math and data enthusiast looking for a position where I can build probabilistic models of complex data in order to inform decision making (with a little bit of help from generative AI). In my most recent jobs, I learned the extract, transform, and load (ETL) process using a variety of methods. I also have experience creating dashboards and KPIs using Tableau and Excel. I believe these experiences uniquely position me to own the data modeling process from end-to-end. Skills: Python, SQL, AWS, Tableau, Excel Passions: Generative AI, probabilistic programming, and data modeling Soft Skills: Concise, empathetic and courteous communication Hobbies: Board games, travel, and food jacob.rider34@gmail.com | LinkedIn | GitHub | Website Experience | Education | Skills | Projects | Contact","title":"Professional Summary"},{"location":"baseball_pitch_predictor/","text":"Baseball Pitch Predictor View Project on GitHub Technologies Used Python Jax Flax Project Overview Created a predictive model to analyze and forecast baseball pitch types based on historical pitch data. Key Features Analysis of pitch frequency over varying time periods to predict the next pitch. Imports import pandas as pd import numpy as np import jax import jax.numpy as jnp from flax import linen as nn from flax.training.train_state import TrainState import optax from jax.random import PRNGKey from parsePitchData import clean_data from sklearn.metrics import confusion_matrix import seaborn as sns import matplotlib.pyplot as plt from typing import Tuple, Dict, List import time Classes PitchPredictorModel class PitchPredictorModel(nn.Module): num_outputs: int # Number of unique pitch types @nn.compact def __call__(self, x: jnp.ndarray) -> jnp.ndarray: \"\"\" Forward pass for the pitch predictor model. Parameters: - x (jnp.ndarray): Input tensor with shape (batch_size, 4, n). Returns: - jnp.ndarray: Logits tensor with shape (batch_size, num_outputs). \"\"\" Functions normalize_counts def normalize_counts(pitch_counts: np.ndarray) -> np.ndarray: \"\"\" Normalize the pitch counts to get a distribution proportion. Parameters: - pitch_counts: Array of pitch counts. Returns: - np.ndarray: Normalized distribution of pitch counts. \"\"\" efficient_pitch_distribution def efficient_pitch_distribution(df: pd.DataFrame, pitch_types: List[str], filter_conditions: Dict[str, str]) -> np.ndarray: \"\"\"Calculate and normalize the distribution of pitch types, excluding the current event. Parameters: - df (pd.DataFrame): The dataframe containing pitch data. - pitch_types (List[str]): List of all possible pitch types. - filter_conditions (Dict[str, str]): Conditions to filter the dataframe. Returns: - np.ndarray: Normalized distribution of pitch counts. \"\"\" return_pitch_distributions def return_pitch_distributions(df: pd.DataFrame, df_row: pd.Series, pitch_types: List[str]) -> jnp.ndarray: \"\"\"Calculate pitch distributions for different contexts based on a row from the dataframe. Parameters: - df (pd.DataFrame): The dataframe containing pitch data. - df_row (pd.Series): A row from the dataframe specifying the current event. - pitch_types (List[str]): List of all possible pitch types. Returns: - jnp.ndarray: Stack of normalized pitch distributions for different contexts. \"\"\" return_current_pitch def return_current_pitch(df_row: pd.Series, pitch_types: List[str]) -> List[int]: \"\"\"Generate a one-hot encoded vector for the current pitch type. Parameters: - df_row (pd.Series): A row from the dataframe specifying the current event. - pitch_types (List[str]): List of all possible pitch types. Returns: - List[int]: One-hot encoded vector representing the current pitch type. \"\"\" clean_data() def clean_data() -> Tuple[jnp.ndarray, jnp.ndarray]: \"\"\"Load, preprocess data, and compute pitch distributions. Returns: - Tuple[jnp.ndarray, jnp.ndarray]: Tuple of inputs and outputs for modeling. \"\"\" cross_entropy_loss() def cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray: \"\"\" Computes the cross-entropy loss between logits and labels. Parameters: - logits (jnp.ndarray): Predictions from model. - labels (jnp.ndarray): True labels, one-hot encoded. Returns: - jnp.ndarray: Cross-entropy loss. \"\"\" compute_accuracy() def compute_accuracy(logits: jnp.ndarray, labels: jnp.ndarray) -> float: \"\"\" Computes the accuracy of the predictions. Parameters: - logits (jnp.ndarray): Predictions from model. - labels (jnp.ndarray): True labels, one-hot encoded. Returns: - float: Accuracy of predictions. \"\"\" create_train_state() def create_train_state(rng: PRNGKey, learning_rate: float, num_outputs: int) -> TrainState: \"\"\" Initializes the model's train state. Parameters: - rng (PRNGKey): Random number generator key. - learning_rate (float): Learning rate for the optimizer. - num_outputs (int): Number of unique pitch types. Returns: - TrainState: The initialized training state. \"\"\" train_step() def train_step(state: TrainState, batch: dict) -> tuple: \"\"\" Performs a single training step. Parameters: - state (TrainState): Current training state. - batch (dict): Batch of data containing 'inputs' and 'targets'. Returns: - tuple: Updated state, loss, and accuracy for the batch. \"\"\" train_model() def train_model(num_epochs: int, learning_rate: float, num_outputs: int, train_data: list) -> None: \"\"\" Trains the model. Parameters: - num_epochs (int): Number of epochs to train for. - learning_rate (float): Learning rate for the optimizer. - num_outputs (int): Number of unique pitch types. - train_data (list): Training data. \"\"\" Example Usage def main(): inputs, outputs = clean_data() # Assuming this returns correctly shaped data n = outputs.shape[1] learning_rate = 0.001 num_epochs = 10 # Placeholder class weights calculation # You should calculate these based on your dataset's specific class distribution class_weights = jnp.array([1.0,5.0,5.0,5.0]) # Example weights for 3 classes # Splitting data into training and testing sets num_training = int(0.8 * len(inputs)) train_inputs, test_inputs = inputs[:num_training], inputs[num_training:] train_outputs, test_outputs = outputs[:num_training], outputs[num_training:] # Initialize the model and training state rng = PRNGKey(0) state = create_train_state(rng, learning_rate, n) # Adjusted training loop for epoch in range(num_epochs): epoch_loss = 0 epoch_accuracy = 0 num_batches = len(train_inputs) # Assuming batch size of 1 for simplicity for i in range(num_batches): inputs = train_inputs[i].reshape(1, 4, n) outputs = train_outputs[i].reshape(1, n) state, loss, accuracy = train_step(state, {'inputs': inputs, 'targets': outputs}) epoch_loss += loss epoch_accuracy += accuracy epoch_loss /= num_batches epoch_accuracy /= num_batches print(f\"Epoch {epoch}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\") # The rest of your code remains unchanged up to the prediction part... # No need to reinitialize the model if we are using the trained state # model = PitchPredictorModel(num_outputs=n) # Remove or comment out this line # Initialize a list to store prediction logits test_pred_logits = [] # Predict in a loop for i in range(len(test_inputs)): # Reshape the input correctly as the model expects test_input_reshaped = test_inputs[i].reshape(1, 4, n) # Correctly use the trained parameters for prediction logits = state.apply_fn({'params': state.params}, test_input_reshaped) test_pred_logits.append(logits) # Assuming test_pred_logits is now a list of arrays, stack them test_pred_logits = jnp.vstack(test_pred_logits) # Stack logits for further processing # Convert logits to predicted class indices test_pred_labels = jnp.argmax(test_pred_logits, axis=-1) # Ensure true_labels is correctly prepared from test_outputs true_labels = jnp.argmax(test_outputs, axis=-1) # Compute the confusion matrix cm = confusion_matrix(true_labels, test_pred_labels) # Plot the confusion matrix plt.figure(figsize=(10, 7)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') plt.xlabel('Predicted Labels') plt.ylabel('True Labels') plt.title('Confusion Matrix') plt.savefig('plots/prediction_results1.png') if __name__ == \"__main__\": main() Experience | Education | Skills | Projects | Contact TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Baseball Pitch Predictor"},{"location":"baseball_pitch_predictor/#baseball-pitch-predictor","text":"View Project on GitHub","title":"Baseball Pitch Predictor"},{"location":"baseball_pitch_predictor/#technologies-used","text":"Python Jax Flax","title":"Technologies Used"},{"location":"baseball_pitch_predictor/#project-overview","text":"Created a predictive model to analyze and forecast baseball pitch types based on historical pitch data.","title":"Project Overview"},{"location":"baseball_pitch_predictor/#key-features","text":"Analysis of pitch frequency over varying time periods to predict the next pitch.","title":"Key Features"},{"location":"baseball_pitch_predictor/#imports","text":"import pandas as pd import numpy as np import jax import jax.numpy as jnp from flax import linen as nn from flax.training.train_state import TrainState import optax from jax.random import PRNGKey from parsePitchData import clean_data from sklearn.metrics import confusion_matrix import seaborn as sns import matplotlib.pyplot as plt from typing import Tuple, Dict, List import time","title":"Imports"},{"location":"baseball_pitch_predictor/#classes","text":"","title":"Classes"},{"location":"baseball_pitch_predictor/#pitchpredictormodel","text":"class PitchPredictorModel(nn.Module): num_outputs: int # Number of unique pitch types @nn.compact def __call__(self, x: jnp.ndarray) -> jnp.ndarray: \"\"\" Forward pass for the pitch predictor model. Parameters: - x (jnp.ndarray): Input tensor with shape (batch_size, 4, n). Returns: - jnp.ndarray: Logits tensor with shape (batch_size, num_outputs). \"\"\"","title":"PitchPredictorModel"},{"location":"baseball_pitch_predictor/#functions","text":"","title":"Functions"},{"location":"baseball_pitch_predictor/#normalize_counts","text":"def normalize_counts(pitch_counts: np.ndarray) -> np.ndarray: \"\"\" Normalize the pitch counts to get a distribution proportion. Parameters: - pitch_counts: Array of pitch counts. Returns: - np.ndarray: Normalized distribution of pitch counts. \"\"\"","title":"normalize_counts"},{"location":"baseball_pitch_predictor/#efficient_pitch_distribution","text":"def efficient_pitch_distribution(df: pd.DataFrame, pitch_types: List[str], filter_conditions: Dict[str, str]) -> np.ndarray: \"\"\"Calculate and normalize the distribution of pitch types, excluding the current event. Parameters: - df (pd.DataFrame): The dataframe containing pitch data. - pitch_types (List[str]): List of all possible pitch types. - filter_conditions (Dict[str, str]): Conditions to filter the dataframe. Returns: - np.ndarray: Normalized distribution of pitch counts. \"\"\"","title":"efficient_pitch_distribution"},{"location":"baseball_pitch_predictor/#return_pitch_distributions","text":"def return_pitch_distributions(df: pd.DataFrame, df_row: pd.Series, pitch_types: List[str]) -> jnp.ndarray: \"\"\"Calculate pitch distributions for different contexts based on a row from the dataframe. Parameters: - df (pd.DataFrame): The dataframe containing pitch data. - df_row (pd.Series): A row from the dataframe specifying the current event. - pitch_types (List[str]): List of all possible pitch types. Returns: - jnp.ndarray: Stack of normalized pitch distributions for different contexts. \"\"\"","title":"return_pitch_distributions"},{"location":"baseball_pitch_predictor/#return_current_pitch","text":"def return_current_pitch(df_row: pd.Series, pitch_types: List[str]) -> List[int]: \"\"\"Generate a one-hot encoded vector for the current pitch type. Parameters: - df_row (pd.Series): A row from the dataframe specifying the current event. - pitch_types (List[str]): List of all possible pitch types. Returns: - List[int]: One-hot encoded vector representing the current pitch type. \"\"\"","title":"return_current_pitch"},{"location":"baseball_pitch_predictor/#clean_data","text":"def clean_data() -> Tuple[jnp.ndarray, jnp.ndarray]: \"\"\"Load, preprocess data, and compute pitch distributions. Returns: - Tuple[jnp.ndarray, jnp.ndarray]: Tuple of inputs and outputs for modeling. \"\"\"","title":"clean_data()"},{"location":"baseball_pitch_predictor/#cross_entropy_loss","text":"def cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray: \"\"\" Computes the cross-entropy loss between logits and labels. Parameters: - logits (jnp.ndarray): Predictions from model. - labels (jnp.ndarray): True labels, one-hot encoded. Returns: - jnp.ndarray: Cross-entropy loss. \"\"\"","title":"cross_entropy_loss()"},{"location":"baseball_pitch_predictor/#compute_accuracy","text":"def compute_accuracy(logits: jnp.ndarray, labels: jnp.ndarray) -> float: \"\"\" Computes the accuracy of the predictions. Parameters: - logits (jnp.ndarray): Predictions from model. - labels (jnp.ndarray): True labels, one-hot encoded. Returns: - float: Accuracy of predictions. \"\"\"","title":"compute_accuracy()"},{"location":"baseball_pitch_predictor/#create_train_state","text":"def create_train_state(rng: PRNGKey, learning_rate: float, num_outputs: int) -> TrainState: \"\"\" Initializes the model's train state. Parameters: - rng (PRNGKey): Random number generator key. - learning_rate (float): Learning rate for the optimizer. - num_outputs (int): Number of unique pitch types. Returns: - TrainState: The initialized training state. \"\"\"","title":"create_train_state()"},{"location":"baseball_pitch_predictor/#train_step","text":"def train_step(state: TrainState, batch: dict) -> tuple: \"\"\" Performs a single training step. Parameters: - state (TrainState): Current training state. - batch (dict): Batch of data containing 'inputs' and 'targets'. Returns: - tuple: Updated state, loss, and accuracy for the batch. \"\"\"","title":"train_step()"},{"location":"baseball_pitch_predictor/#train_model","text":"def train_model(num_epochs: int, learning_rate: float, num_outputs: int, train_data: list) -> None: \"\"\" Trains the model. Parameters: - num_epochs (int): Number of epochs to train for. - learning_rate (float): Learning rate for the optimizer. - num_outputs (int): Number of unique pitch types. - train_data (list): Training data. \"\"\"","title":"train_model()"},{"location":"baseball_pitch_predictor/#example-usage","text":"def main(): inputs, outputs = clean_data() # Assuming this returns correctly shaped data n = outputs.shape[1] learning_rate = 0.001 num_epochs = 10 # Placeholder class weights calculation # You should calculate these based on your dataset's specific class distribution class_weights = jnp.array([1.0,5.0,5.0,5.0]) # Example weights for 3 classes # Splitting data into training and testing sets num_training = int(0.8 * len(inputs)) train_inputs, test_inputs = inputs[:num_training], inputs[num_training:] train_outputs, test_outputs = outputs[:num_training], outputs[num_training:] # Initialize the model and training state rng = PRNGKey(0) state = create_train_state(rng, learning_rate, n) # Adjusted training loop for epoch in range(num_epochs): epoch_loss = 0 epoch_accuracy = 0 num_batches = len(train_inputs) # Assuming batch size of 1 for simplicity for i in range(num_batches): inputs = train_inputs[i].reshape(1, 4, n) outputs = train_outputs[i].reshape(1, n) state, loss, accuracy = train_step(state, {'inputs': inputs, 'targets': outputs}) epoch_loss += loss epoch_accuracy += accuracy epoch_loss /= num_batches epoch_accuracy /= num_batches print(f\"Epoch {epoch}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\") # The rest of your code remains unchanged up to the prediction part... # No need to reinitialize the model if we are using the trained state # model = PitchPredictorModel(num_outputs=n) # Remove or comment out this line # Initialize a list to store prediction logits test_pred_logits = [] # Predict in a loop for i in range(len(test_inputs)): # Reshape the input correctly as the model expects test_input_reshaped = test_inputs[i].reshape(1, 4, n) # Correctly use the trained parameters for prediction logits = state.apply_fn({'params': state.params}, test_input_reshaped) test_pred_logits.append(logits) # Assuming test_pred_logits is now a list of arrays, stack them test_pred_logits = jnp.vstack(test_pred_logits) # Stack logits for further processing # Convert logits to predicted class indices test_pred_labels = jnp.argmax(test_pred_logits, axis=-1) # Ensure true_labels is correctly prepared from test_outputs true_labels = jnp.argmax(test_outputs, axis=-1) # Compute the confusion matrix cm = confusion_matrix(true_labels, test_pred_labels) # Plot the confusion matrix plt.figure(figsize=(10, 7)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') plt.xlabel('Predicted Labels') plt.ylabel('True Labels') plt.title('Confusion Matrix') plt.savefig('plots/prediction_results1.png') if __name__ == \"__main__\": main() Experience | Education | Skills | Projects | Contact TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Example Usage"},{"location":"contact/","text":"Contact Details jacob.rider34@gmail.com LinkedIn GitHub Website Experience | Education | Skills | Projects | Contact","title":"Contact"},{"location":"contact/#contact-details","text":"jacob.rider34@gmail.com LinkedIn GitHub Website Experience | Education | Skills | Projects | Contact","title":"Contact Details"},{"location":"data_tools/","text":"Data Tools for Economic Research View Project on GitHub Technologies Used Python SQL Project Overview Developed a suite of data analysis tools tailored for economic research, facilitating the efficient cleaning, processing, and ingestion of economic and financial API data into a SQLite database. Key Features Automated data cleaning and processing workflows. Ingestion of diverse economic and financial data into a unified database. Simplified access to clean, processed data for economic research and analysis. Imports import sqlite3 import requests import pandas as pd from sklearn.preprocessing import StandardScaler from typing import List, Optional Classes DataIngestor class DataIngestor: \"\"\" A class to ingest and process financial data from a SQLite database. \"\"\" def __init__(self, db_path: str, table_names: list, min_date: str): \"\"\" Initialize the DataIngestor with database path, table names, and minimum date. Args: db_path (str): Path to the SQLite database. table_names (list): List of table names to process. min_date (str): Minimum date for filtering data. \"\"\" self.db_path = db_path self.table_names = table_names self.min_date = min_date DataFetcher class DataFetcher: \"\"\" A class to fetch and insert financial and economic data into a SQLite database. Attributes: db_path (str): The file path for the SQLite database. \"\"\" def __init__(self, db_path: str) -> None: \"\"\" Initialize the DataFetcher class. Args: db_path (str): The file path for the SQLite database. \"\"\" self.db_path = db_path Methods DataFetcher Methods create_table() def create_table(self, table_name: str, columns: List[str], is_stock: bool) -> None: \"\"\" Create a table in the SQLite database. Args: table_name (str): The name of the table to create. columns (List[str]): A list of column names for the table. isStock (bool): Indicator of whether the data is stock data (True) or economic data (False). \"\"\" fetch_and_insert_stock_data() def fetch_and_insert_stock_data( self, stock_ticker: str, api_key: str, report_link: str, from_date: str, to_date: str, ) -> None: \"\"\" Fetch stock data from an API and insert it into the database. Args: stock_ticker (str): The stock ticker symbol. api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. from_date (str): Start date for the data. to_date (str): End date for the data. \"\"\" fetch_and_insert_economic_data() def fetch_and_insert_economic_data(self, api_key: str, report_link: str) -> None: \"\"\" Fetch economic data from an API and insert it into the database. Args: api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. \"\"\" _insert_data() def _insert_data( self, table_name: str, columns: List[str], data: List[dict], ticker: Optional[str] = None, ) -> None: \"\"\" Insert data into a specified table in the database. Args: table_name (str): The name of the table to insert data into. columns (List[str]): The column names for data insertion. data (List[Dict]): The data to be inserted. ticker (Optional[str]): The stock ticker symbol. Defaults to None. \"\"\" DataIngestor Methods get_column_names() def get_column_names(self, conn: sqlite3.Connection, table_name: str) -> list: \"\"\" Retrieve column names for a given table in the database. Args: conn (sqlite3.Connection): A connection object to the SQLite database. table_name (str): Name of the table to retrieve columns from. Returns: list: A list of column names. \"\"\" fetch_and_process_data() def fetch_and_process_data(self) -> list: \"\"\" Fetch and process data from the database for each table. Returns: list: A list of processed pandas DataFrames. \"\"\" scale_data() def scale_data(self, dfs: list) -> pd.DataFrame: \"\"\" Scale the data using StandardScaler. Args: dfs (list): List of pandas DataFrames to scale. Returns: pd.DataFrame: A DataFrame of scaled features. \"\"\" utils process_data() def process_data(db_path: str, table_names: list, min_date: str) -> tuple: \"\"\" Process and scale data from the database. Args: db_path (str): Path to the SQLite database. table_names (list): List of table names to process. min_date (str): Minimum date for filtering data. Returns: tuple: A tuple containing list of DataFrames and scaled DataFrame. \"\"\" drop_table() def drop_table(db_path: str, table_name: str) -> None: \"\"\" Drop a table from the database. Args: db_path (str): Path to the SQLite database. table_name (str): Name of the table to be dropped. \"\"\" fetch_stocks_data() def fetch_stocks_data( db_path: str, stocks: List[str], api_key: str, report_link: str ) -> None: \"\"\" Fetch and insert stock data for multiple stocks into the database. Args: db_path (str): The file path for the SQLite database. stocks (List[str]): A list of stock ticker symbols. api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. fetch_economic_data() def fetch_economic_data(db_path: str, table_names: List[str], api_key: str) -> None: \"\"\" Fetch and insert economic data for multiple series into the database. Args: db_path (str): The file path for the SQLite database. table_names (list[str]): A list of economic data series identifiers. api_key (str): The API key for authentication. \"\"\" Experience | Education | Skills | Projects | Contact TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Data Tools for Economic Research"},{"location":"data_tools/#data-tools-for-economic-research","text":"View Project on GitHub","title":"Data Tools for Economic Research"},{"location":"data_tools/#technologies-used","text":"Python SQL","title":"Technologies Used"},{"location":"data_tools/#project-overview","text":"Developed a suite of data analysis tools tailored for economic research, facilitating the efficient cleaning, processing, and ingestion of economic and financial API data into a SQLite database.","title":"Project Overview"},{"location":"data_tools/#key-features","text":"Automated data cleaning and processing workflows. Ingestion of diverse economic and financial data into a unified database. Simplified access to clean, processed data for economic research and analysis.","title":"Key Features"},{"location":"data_tools/#imports","text":"import sqlite3 import requests import pandas as pd from sklearn.preprocessing import StandardScaler from typing import List, Optional","title":"Imports"},{"location":"data_tools/#classes","text":"","title":"Classes"},{"location":"data_tools/#dataingestor","text":"class DataIngestor: \"\"\" A class to ingest and process financial data from a SQLite database. \"\"\" def __init__(self, db_path: str, table_names: list, min_date: str): \"\"\" Initialize the DataIngestor with database path, table names, and minimum date. Args: db_path (str): Path to the SQLite database. table_names (list): List of table names to process. min_date (str): Minimum date for filtering data. \"\"\" self.db_path = db_path self.table_names = table_names self.min_date = min_date","title":"DataIngestor"},{"location":"data_tools/#datafetcher","text":"class DataFetcher: \"\"\" A class to fetch and insert financial and economic data into a SQLite database. Attributes: db_path (str): The file path for the SQLite database. \"\"\" def __init__(self, db_path: str) -> None: \"\"\" Initialize the DataFetcher class. Args: db_path (str): The file path for the SQLite database. \"\"\" self.db_path = db_path","title":"DataFetcher"},{"location":"data_tools/#methods","text":"","title":"Methods"},{"location":"data_tools/#datafetcher-methods","text":"","title":"DataFetcher Methods"},{"location":"data_tools/#create_table","text":"def create_table(self, table_name: str, columns: List[str], is_stock: bool) -> None: \"\"\" Create a table in the SQLite database. Args: table_name (str): The name of the table to create. columns (List[str]): A list of column names for the table. isStock (bool): Indicator of whether the data is stock data (True) or economic data (False). \"\"\"","title":"create_table()"},{"location":"data_tools/#fetch_and_insert_stock_data","text":"def fetch_and_insert_stock_data( self, stock_ticker: str, api_key: str, report_link: str, from_date: str, to_date: str, ) -> None: \"\"\" Fetch stock data from an API and insert it into the database. Args: stock_ticker (str): The stock ticker symbol. api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. from_date (str): Start date for the data. to_date (str): End date for the data. \"\"\"","title":"fetch_and_insert_stock_data()"},{"location":"data_tools/#fetch_and_insert_economic_data","text":"def fetch_and_insert_economic_data(self, api_key: str, report_link: str) -> None: \"\"\" Fetch economic data from an API and insert it into the database. Args: api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request. \"\"\"","title":"fetch_and_insert_economic_data()"},{"location":"data_tools/#_insert_data","text":"def _insert_data( self, table_name: str, columns: List[str], data: List[dict], ticker: Optional[str] = None, ) -> None: \"\"\" Insert data into a specified table in the database. Args: table_name (str): The name of the table to insert data into. columns (List[str]): The column names for data insertion. data (List[Dict]): The data to be inserted. ticker (Optional[str]): The stock ticker symbol. Defaults to None. \"\"\"","title":"_insert_data()"},{"location":"data_tools/#dataingestor-methods","text":"","title":"DataIngestor Methods"},{"location":"data_tools/#get_column_names","text":"def get_column_names(self, conn: sqlite3.Connection, table_name: str) -> list: \"\"\" Retrieve column names for a given table in the database. Args: conn (sqlite3.Connection): A connection object to the SQLite database. table_name (str): Name of the table to retrieve columns from. Returns: list: A list of column names. \"\"\"","title":"get_column_names()"},{"location":"data_tools/#fetch_and_process_data","text":"def fetch_and_process_data(self) -> list: \"\"\" Fetch and process data from the database for each table. Returns: list: A list of processed pandas DataFrames. \"\"\"","title":"fetch_and_process_data()"},{"location":"data_tools/#scale_data","text":"def scale_data(self, dfs: list) -> pd.DataFrame: \"\"\" Scale the data using StandardScaler. Args: dfs (list): List of pandas DataFrames to scale. Returns: pd.DataFrame: A DataFrame of scaled features. \"\"\"","title":"scale_data()"},{"location":"data_tools/#utils","text":"","title":"utils"},{"location":"data_tools/#process_data","text":"def process_data(db_path: str, table_names: list, min_date: str) -> tuple: \"\"\" Process and scale data from the database. Args: db_path (str): Path to the SQLite database. table_names (list): List of table names to process. min_date (str): Minimum date for filtering data. Returns: tuple: A tuple containing list of DataFrames and scaled DataFrame. \"\"\"","title":"process_data()"},{"location":"data_tools/#drop_table","text":"def drop_table(db_path: str, table_name: str) -> None: \"\"\" Drop a table from the database. Args: db_path (str): Path to the SQLite database. table_name (str): Name of the table to be dropped. \"\"\"","title":"drop_table()"},{"location":"data_tools/#fetch_stocks_data","text":"def fetch_stocks_data( db_path: str, stocks: List[str], api_key: str, report_link: str ) -> None: \"\"\" Fetch and insert stock data for multiple stocks into the database. Args: db_path (str): The file path for the SQLite database. stocks (List[str]): A list of stock ticker symbols. api_key (str): The API key for authentication. report_link (str): The endpoint link for the API request.","title":"fetch_stocks_data()"},{"location":"data_tools/#fetch_economic_data","text":"def fetch_economic_data(db_path: str, table_names: List[str], api_key: str) -> None: \"\"\" Fetch and insert economic data for multiple series into the database. Args: db_path (str): The file path for the SQLite database. table_names (list[str]): A list of economic data series identifiers. api_key (str): The API key for authentication. \"\"\" Experience | Education | Skills | Projects | Contact TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"fetch_economic_data()"},{"location":"education/","text":"Bachelors of Science Mathematics and Economics Georgia State University, Atlanta, GA | Class of 2020 Completed a comprehensive curriculum focusing on rigorous math courses, quantitative analysis, statistical methods, and economic theory. Led a team project modeling asset bubbles with physics equations, using Bloomberg data for model calibration. Collaborated with the Molkov Theoretical Neuroscience Research Group by developing visualizations for experimental data using Matplotlib and Python to model the feedback mechanisms that underlie the cardio-respiratory system. Certifications Tableau Data Analyst Tableau Desktop Specialist Experience | Education | Skills | Projects | Contact","title":"Education"},{"location":"education/#bachelors-of-science","text":"","title":"Bachelors of Science"},{"location":"education/#mathematics-and-economics","text":"Georgia State University, Atlanta, GA | Class of 2020 Completed a comprehensive curriculum focusing on rigorous math courses, quantitative analysis, statistical methods, and economic theory. Led a team project modeling asset bubbles with physics equations, using Bloomberg data for model calibration. Collaborated with the Molkov Theoretical Neuroscience Research Group by developing visualizations for experimental data using Matplotlib and Python to model the feedback mechanisms that underlie the cardio-respiratory system.","title":"Mathematics and Economics"},{"location":"education/#certifications","text":"Tableau Data Analyst Tableau Desktop Specialist Experience | Education | Skills | Projects | Contact","title":"Certifications"},{"location":"experience/","text":"Associate Data Engineer Delta Air Lines Atlanta, GA | June 2022 \u2013 Present Developed and maintained scalable data pipelines and implemented ETL processes to support analytics and business intelligence (BI) reporting. Collaborated with cross-functional teams to understand data needs, leading the design and implementation of Tableau dashboards. Utilized Python and SQL for data manipulation, cleaning, and analysis, improving data accuracy and utility across departments. Played a key role in the migration of legacy systems to AWS, ensuring minimal downtime and data integrity. Logistics Document Specialist Mercedes-Benz USA, LLC Atlanta, GA | March 2021 \u2013 December 2021 Managed and streamlined the documentation process for logistics operations, enhancing the efficiency of shipment and delivery schedules. Created VBA macros that query the SAP HANA database and automatically generate KPIs in order to drive monthly performance reviews for each carrier. Built and implemented a Python script to parse through emails based on a given set of keywords in order to improve data quality and reduce time spent aggregating VINs and PO numbers. Automated routine management reports using Excel, VBA, SAP, and Python saving approximately 50 hours per month for key logistics personnel. Contributed to the development of training materials and procedures for new team members, ensuring consistent and accurate document handling. Experience | Education | Skills | Projects | Contact","title":"Experience"},{"location":"experience/#associate-data-engineer","text":"","title":"Associate Data Engineer"},{"location":"experience/#delta-air-lines","text":"Atlanta, GA | June 2022 \u2013 Present Developed and maintained scalable data pipelines and implemented ETL processes to support analytics and business intelligence (BI) reporting. Collaborated with cross-functional teams to understand data needs, leading the design and implementation of Tableau dashboards. Utilized Python and SQL for data manipulation, cleaning, and analysis, improving data accuracy and utility across departments. Played a key role in the migration of legacy systems to AWS, ensuring minimal downtime and data integrity.","title":"Delta Air Lines"},{"location":"experience/#logistics-document-specialist","text":"","title":"Logistics Document Specialist"},{"location":"experience/#mercedes-benz-usa-llc","text":"Atlanta, GA | March 2021 \u2013 December 2021 Managed and streamlined the documentation process for logistics operations, enhancing the efficiency of shipment and delivery schedules. Created VBA macros that query the SAP HANA database and automatically generate KPIs in order to drive monthly performance reviews for each carrier. Built and implemented a Python script to parse through emails based on a given set of keywords in order to improve data quality and reduce time spent aggregating VINs and PO numbers. Automated routine management reports using Excel, VBA, SAP, and Python saving approximately 50 hours per month for key logistics personnel. Contributed to the development of training materials and procedures for new team members, ensuring consistent and accurate document handling. Experience | Education | Skills | Projects | Contact","title":"Mercedes-Benz USA, LLC"},{"location":"prob_prog_tools/","text":"Probabilistic Programming Tools View Project on GitHub Technologies Used Python NumPy Jax NumPyro Project Overview Designed tools that simulate a random levy process and then uses NumPyro to infer the parameters used to simulate those processes. Key Features Simulation of processes that resemble stock price movements using random levy processes. Parameter inference with NumPyro to understand underlying market dynamics. Imports from scipy.stats import levy_stable from jax.scipy.linalg import cholesky from typing import Dict, List, Tuple import os from typing import Any, Dict import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpyro import numpyro.distributions as dist import seaborn as sns from numpyro.infer import MCMC, NUTS Classes StockMarketSimulator class StockMarketSimulator: \"\"\" A simulator for stock market prices using Levy processes with JAX for computation and Pareto-distributed initial prices using NumPy. Attributes: n_industries (int): Number of industries. n_stocks_per_industry (int): Number of stocks per industry. base_stock_price (float): Base stock price for scaling initial prices. industries (List[str]): List of industry names. stocks (List[str]): List of stock symbols. stock_prices (pd.DataFrame): DataFrame to store simulated stock prices. seed (int): Int derived through os for true random state key (jax.random.PRNGKey): JAX PRNG key for random number generation. industry_map (Dict[str, str]): Mapping of stocks to their respective industries. alpha_params (Dict[str, float]): Alpha parameter for each industry. beta_params (Dict[str, float]): Beta parameter for each industry. pareto_shapes (Dict[str, float]): Pareto shape parameter for each industry. \"\"\" Methods init () def __init__(self, n_industries: int = 8, n_stocks_per_industry: int = 10, base_stock_price: float = 100) -> None: \"\"\" Initialize the stock market simulator with specified parameters. Args: n_industries (int): The number of industries to simulate. n_stocks_per_industry (int): The number of stocks per industry. base_stock_price (float): The base stock price for scaling initial prices. \"\"\" _initialize_prices() def _initialize_prices(self) -> None: \"\"\"Initialize the stock prices using Pareto distribution for each industry.\"\"\" _simulate_stock_prices() def simulate_stock_prices(self, n_days: int = 252) -> pd.DataFrame: \"\"\" Simulate stock prices over a given number of days. Args: n_days (int): The number of days to simulate stock prices. Returns: pd.DataFrame: A DataFrame containing the simulated stock prices. \"\"\" _apply_correlation def _apply_correlation(self, increments: np.ndarray) -> np.ndarray: \"\"\" Apply a correlation matrix to the increments using Cholesky decomposition. Args: increments (np.ndarray): An array of increments to apply correlation to. Returns: np.ndarray: Correlated increments after applying the correlation matrix. \"\"\" Functions plot_posteriors() def plot_posteriors(posterior_samples: Dict[str, jnp.ndarray], industry: str) -> None: \"\"\" Plots the posterior distributions for a given industry. Args: posterior_samples: Samples from the posterior distribution as a dictionary where keys are parameter names. industry (str): The name of the industry for which the posterior distributions are plotted. \"\"\" run_bayesian_inference() def run_bayesian_inference(simulator: DI.StockMarketSimulator, n_samples: int = 500, n_warmup: int = 100) -> None: \"\"\" Runs Bayesian inference for each industry and plots the posterior distributions. Args: simulator: An instance of StockMarketSimulator containing stock prices and industry mappings. n_samples (int): Number of samples to draw from the posterior distribution. n_warmup (int): Number of warmup steps for the sampler. \"\"\" plot_stock_prices() def plot_stock_prices(simulator: DI.StockMarketSimulator) -> None: \"\"\" Plot the simulated stock prices for all stocks in the simulation. Args: simulator (DI.StockMarketSimulator): An instance of the StockMarketSimulator class. This function generates a line plot for each stock across the simulated days and saves the plot to a file named 'simulated_stock_prices.png' in the 'plots' directory. \"\"\" plot_stock_prices_by_industry() def plot_stock_prices_by_industry(simulator: DI.StockMarketSimulator) -> None: \"\"\" Plot the simulated stock prices for each industry separately. Args: simulator (DI.StockMarketSimulator): An instance of the StockMarketSimulator class. This function generates a line plot for each stock within an industry across the simulated days and saves each industry's plot to a separate file in the 'plots' directory, named 'simulated_stock_price_by_[industry].png'. \"\"\" Example Usage def main() -> None: \"\"\" Main function to initialize the simulator, run the stock price simulation, and plot the results. \"\"\" simulator = DI.StockMarketSimulator() run_bayesian_inference(simulator) simulator.simulate_stock_prices() plot_stock_prices(simulator) plot_stock_prices_by_industry(simulator) if __name__ == \"__main__\": main() Experience | Education | Skills | Projects | Contact TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Probabilistic Programming Tools"},{"location":"prob_prog_tools/#probabilistic-programming-tools","text":"View Project on GitHub","title":"Probabilistic Programming Tools"},{"location":"prob_prog_tools/#technologies-used","text":"Python NumPy Jax NumPyro","title":"Technologies Used"},{"location":"prob_prog_tools/#project-overview","text":"Designed tools that simulate a random levy process and then uses NumPyro to infer the parameters used to simulate those processes.","title":"Project Overview"},{"location":"prob_prog_tools/#key-features","text":"Simulation of processes that resemble stock price movements using random levy processes. Parameter inference with NumPyro to understand underlying market dynamics.","title":"Key Features"},{"location":"prob_prog_tools/#imports","text":"from scipy.stats import levy_stable from jax.scipy.linalg import cholesky from typing import Dict, List, Tuple import os from typing import Any, Dict import jax import jax.numpy as jnp import matplotlib.pyplot as plt import numpyro import numpyro.distributions as dist import seaborn as sns from numpyro.infer import MCMC, NUTS","title":"Imports"},{"location":"prob_prog_tools/#classes","text":"","title":"Classes"},{"location":"prob_prog_tools/#stockmarketsimulator","text":"class StockMarketSimulator: \"\"\" A simulator for stock market prices using Levy processes with JAX for computation and Pareto-distributed initial prices using NumPy. Attributes: n_industries (int): Number of industries. n_stocks_per_industry (int): Number of stocks per industry. base_stock_price (float): Base stock price for scaling initial prices. industries (List[str]): List of industry names. stocks (List[str]): List of stock symbols. stock_prices (pd.DataFrame): DataFrame to store simulated stock prices. seed (int): Int derived through os for true random state key (jax.random.PRNGKey): JAX PRNG key for random number generation. industry_map (Dict[str, str]): Mapping of stocks to their respective industries. alpha_params (Dict[str, float]): Alpha parameter for each industry. beta_params (Dict[str, float]): Beta parameter for each industry. pareto_shapes (Dict[str, float]): Pareto shape parameter for each industry. \"\"\"","title":"StockMarketSimulator"},{"location":"prob_prog_tools/#methods","text":"","title":"Methods"},{"location":"prob_prog_tools/#init","text":"def __init__(self, n_industries: int = 8, n_stocks_per_industry: int = 10, base_stock_price: float = 100) -> None: \"\"\" Initialize the stock market simulator with specified parameters. Args: n_industries (int): The number of industries to simulate. n_stocks_per_industry (int): The number of stocks per industry. base_stock_price (float): The base stock price for scaling initial prices. \"\"\"","title":"init()"},{"location":"prob_prog_tools/#_initialize_prices","text":"def _initialize_prices(self) -> None: \"\"\"Initialize the stock prices using Pareto distribution for each industry.\"\"\"","title":"_initialize_prices()"},{"location":"prob_prog_tools/#_simulate_stock_prices","text":"def simulate_stock_prices(self, n_days: int = 252) -> pd.DataFrame: \"\"\" Simulate stock prices over a given number of days. Args: n_days (int): The number of days to simulate stock prices. Returns: pd.DataFrame: A DataFrame containing the simulated stock prices. \"\"\"","title":"_simulate_stock_prices()"},{"location":"prob_prog_tools/#_apply_correlation","text":"def _apply_correlation(self, increments: np.ndarray) -> np.ndarray: \"\"\" Apply a correlation matrix to the increments using Cholesky decomposition. Args: increments (np.ndarray): An array of increments to apply correlation to. Returns: np.ndarray: Correlated increments after applying the correlation matrix. \"\"\"","title":"_apply_correlation"},{"location":"prob_prog_tools/#functions","text":"","title":"Functions"},{"location":"prob_prog_tools/#plot_posteriors","text":"def plot_posteriors(posterior_samples: Dict[str, jnp.ndarray], industry: str) -> None: \"\"\" Plots the posterior distributions for a given industry. Args: posterior_samples: Samples from the posterior distribution as a dictionary where keys are parameter names. industry (str): The name of the industry for which the posterior distributions are plotted. \"\"\"","title":"plot_posteriors()"},{"location":"prob_prog_tools/#run_bayesian_inference","text":"def run_bayesian_inference(simulator: DI.StockMarketSimulator, n_samples: int = 500, n_warmup: int = 100) -> None: \"\"\" Runs Bayesian inference for each industry and plots the posterior distributions. Args: simulator: An instance of StockMarketSimulator containing stock prices and industry mappings. n_samples (int): Number of samples to draw from the posterior distribution. n_warmup (int): Number of warmup steps for the sampler. \"\"\"","title":"run_bayesian_inference()"},{"location":"prob_prog_tools/#plot_stock_prices","text":"def plot_stock_prices(simulator: DI.StockMarketSimulator) -> None: \"\"\" Plot the simulated stock prices for all stocks in the simulation. Args: simulator (DI.StockMarketSimulator): An instance of the StockMarketSimulator class. This function generates a line plot for each stock across the simulated days and saves the plot to a file named 'simulated_stock_prices.png' in the 'plots' directory. \"\"\"","title":"plot_stock_prices()"},{"location":"prob_prog_tools/#plot_stock_prices_by_industry","text":"def plot_stock_prices_by_industry(simulator: DI.StockMarketSimulator) -> None: \"\"\" Plot the simulated stock prices for each industry separately. Args: simulator (DI.StockMarketSimulator): An instance of the StockMarketSimulator class. This function generates a line plot for each stock within an industry across the simulated days and saves each industry's plot to a separate file in the 'plots' directory, named 'simulated_stock_price_by_[industry].png'. \"\"\"","title":"plot_stock_prices_by_industry()"},{"location":"prob_prog_tools/#example-usage","text":"def main() -> None: \"\"\" Main function to initialize the simulator, run the stock price simulation, and plot the results. \"\"\" simulator = DI.StockMarketSimulator() run_bayesian_inference(simulator) simulator.simulate_stock_prices() plot_stock_prices(simulator) plot_stock_prices_by_industry(simulator) if __name__ == \"__main__\": main() Experience | Education | Skills | Projects | Contact TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Example Usage"},{"location":"projects/","text":"Projects TI4 Combat Simulator Technologies Used : Python, Jax, NumPyro Developed a combat simulator for the board game \"Twilight Imperium 4th Edition\" (TI4), utilizing Monte Carlo and probability theory to model the distribution of outcomes for in-game combat scenarios. Baseball Pitch Predictor Technologies Used : Python, Jax, Flax Created a predictive model to analyze and forecast baseball pitch types based on historical pitch data. Probabilistic Programming Tools Technologies Used : Python, NumPy, Jax, NumPyro Designed a stock market simulation tool that simulates a random levy process and then uses NumPyro infers the parameters used to simulate those processes. Data Tools for Economic Research Technologies Used : Python, SQL Developed a suite of data analysis tools tailored for economic research and the cleaning and ingestion of economic and financial API data into a SQLite database. Experience | Education | Skills | Projects | Contact","title":"Projects"},{"location":"projects/#projects","text":"","title":"Projects"},{"location":"projects/#ti4-combat-simulator","text":"Technologies Used : Python, Jax, NumPyro Developed a combat simulator for the board game \"Twilight Imperium 4th Edition\" (TI4), utilizing Monte Carlo and probability theory to model the distribution of outcomes for in-game combat scenarios.","title":"TI4 Combat Simulator"},{"location":"projects/#baseball-pitch-predictor","text":"Technologies Used : Python, Jax, Flax Created a predictive model to analyze and forecast baseball pitch types based on historical pitch data.","title":"Baseball Pitch Predictor"},{"location":"projects/#probabilistic-programming-tools","text":"Technologies Used : Python, NumPy, Jax, NumPyro Designed a stock market simulation tool that simulates a random levy process and then uses NumPyro infers the parameters used to simulate those processes.","title":"Probabilistic Programming Tools"},{"location":"projects/#data-tools-for-economic-research","text":"Technologies Used : Python, SQL Developed a suite of data analysis tools tailored for economic research and the cleaning and ingestion of economic and financial API data into a SQLite database. Experience | Education | Skills | Projects | Contact","title":"Data Tools for Economic Research"},{"location":"skills/","text":"Skills Python : Proficient in Python programming for end-to-end data processing and modeling Pandas NumPy Jax NumPyro SQL : Experienced in SQL for database management, querying, and data manipulation AWS : Familiarity with AWS services such as lambda, s3, and athena Tableau : Skilled in using Tableau to inform decision makers with visually appealing and insightful dashboards Excel : Advanced proficiency in Excel Passions Probabilistic Programming : Fascinated by the application of probabilistic programming for building complex models and simulations Generative AI : Interested in exploring the capabilities of generative AI for creating new content and solving unique problems Data Modeling : Passionate about constructing accurate data models to uncover insights and inform strategic decision-making Experience | Education | Skills | Projects | Contact","title":"Skills"},{"location":"skills/#skills","text":"Python : Proficient in Python programming for end-to-end data processing and modeling Pandas NumPy Jax NumPyro SQL : Experienced in SQL for database management, querying, and data manipulation AWS : Familiarity with AWS services such as lambda, s3, and athena Tableau : Skilled in using Tableau to inform decision makers with visually appealing and insightful dashboards Excel : Advanced proficiency in Excel","title":"Skills"},{"location":"skills/#passions","text":"Probabilistic Programming : Fascinated by the application of probabilistic programming for building complex models and simulations Generative AI : Interested in exploring the capabilities of generative AI for creating new content and solving unique problems Data Modeling : Passionate about constructing accurate data models to uncover insights and inform strategic decision-making Experience | Education | Skills | Projects | Contact","title":"Passions"},{"location":"ti4_combat_simulator/","text":"TI4 Combat Simulator View Project on GitHub Technologies Used Python Jax Project Background Twilight Imperium (TI4) is a grand strategy board game known for its intricate diplomacy, expansive empire-building, and epic space battles. TI4's combat involves a blend of strategic planning and randomness. While unit statistics and player decisions play a significant role, the combat system incorporates elements of randomness through dice rolls. Project Overview This combat simulator for TI4 utilizes Monte Carlo and probability theory to model the distribution of outcomes for in-game combat scenarios. This tool helps players understand the potential outcomes of their strategic decisions in combat, aiding in planning and execution during gameplay. Key Features Monte Carlo simulation methods to predict combat outcomes. Probability theory to sample from combinations of probabilities. ti4_functions This document describes a set of functions for simulating combat scenarios using JAX. Imports import jax import jax.numpy as jnp from jax import random import os from typing import Dict, Tuple, Any Types KeyType = Tuple[int, float] SideType = Dict[KeyType, int] RNGKey = Any Functions apply_hits def apply_hits(side: SideType, hits_scored: int, rng_key: RNGKey) -> SideType: \"\"\" Apply hits to a side with JAX, prioritizing dice with lower hit probabilities and lower health. Incorporates randomness in selecting dice within the same priority level to take hits. Accepts an RNG key for reproducible randomness. Parameters: side (SideType): Dictionary representing the side's units and their stats. hits_scored (int): Number of hits to apply to the side. rng_key (RNGKey): JAX random key for generating random numbers. Returns: SideType: Updated side after applying hits. \"\"\" simulate_combat_round_jax def simulate_combat_round_jax(side_a: SideType, side_b: SideType, rng_key: RNGKey) -> Tuple[SideType, SideType]: \"\"\" Simulates a single round of combat between two sides using JAX. Parameters: side_a (SideType): First combatant side. side_b (SideType): Second combatant side. rng_key (RNGKey): JAX random key for generating random numbers. Returns: Tuple[SideType, SideType]: Updated states of side_a and side_b after combat. \"\"\" run_combat_until_elimination def simulate_combat_round_jax(side_a: SideType, side_b: SideType, rng_key: RNGKey) -> Tuple[SideType, SideType]: \"\"\" Simulates a single round of combat between two sides using JAX. Parameters: side_a (SideType): First combatant side. side_b (SideType): Second combatant side. rng_key (RNGKey): JAX random key for generating random numbers. Returns: Tuple[SideType, SideType]: Updated states of side_a and side_b after combat. \"\"\" monte_carlo_combat_simulation def monte_carlo_combat_simulation(initial_side_a: SideType, initial_side_b: SideType, num_simulations: int = 1000) -> Dict[str, float]: \"\"\" Performs a Monte Carlo simulation of combat between two sides over a specified number of simulations to estimate outcome probabilities. Parameters: initial_side_a (SideType): Initial state of side A. initial_side_b (SideType): Initial state of side B. num_simulations (int): Number of simulations to run. Returns: Dict[str, float]: Probabilities of different outcomes. \"\"\" Example Usage The following is an example of 2 dreadnaughts, each with 2 health hitting 60% of the time fighting vs 6 fighters, each with 1 health hitting 20% of the time. initial_side_a = {(2, 0.6): 3} initial_side_b = {(1, 0.2): 6} num_simulations = 10000 probabilities = monte_carlo_combat_simulation(initial_side_a, initial_side_b, num_simulations) print(probabilities) Experience | Education | Skills | Projects | Contact TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"TI4 Combat Simulator"},{"location":"ti4_combat_simulator/#ti4-combat-simulator","text":"View Project on GitHub","title":"TI4 Combat Simulator"},{"location":"ti4_combat_simulator/#technologies-used","text":"Python Jax","title":"Technologies Used"},{"location":"ti4_combat_simulator/#project-background","text":"Twilight Imperium (TI4) is a grand strategy board game known for its intricate diplomacy, expansive empire-building, and epic space battles. TI4's combat involves a blend of strategic planning and randomness. While unit statistics and player decisions play a significant role, the combat system incorporates elements of randomness through dice rolls.","title":"Project Background"},{"location":"ti4_combat_simulator/#project-overview","text":"This combat simulator for TI4 utilizes Monte Carlo and probability theory to model the distribution of outcomes for in-game combat scenarios. This tool helps players understand the potential outcomes of their strategic decisions in combat, aiding in planning and execution during gameplay.","title":"Project Overview"},{"location":"ti4_combat_simulator/#key-features","text":"Monte Carlo simulation methods to predict combat outcomes. Probability theory to sample from combinations of probabilities.","title":"Key Features"},{"location":"ti4_combat_simulator/#ti4_functions","text":"This document describes a set of functions for simulating combat scenarios using JAX.","title":"ti4_functions"},{"location":"ti4_combat_simulator/#imports","text":"import jax import jax.numpy as jnp from jax import random import os from typing import Dict, Tuple, Any","title":"Imports"},{"location":"ti4_combat_simulator/#types","text":"KeyType = Tuple[int, float] SideType = Dict[KeyType, int] RNGKey = Any","title":"Types"},{"location":"ti4_combat_simulator/#functions","text":"","title":"Functions"},{"location":"ti4_combat_simulator/#apply_hits","text":"def apply_hits(side: SideType, hits_scored: int, rng_key: RNGKey) -> SideType: \"\"\" Apply hits to a side with JAX, prioritizing dice with lower hit probabilities and lower health. Incorporates randomness in selecting dice within the same priority level to take hits. Accepts an RNG key for reproducible randomness. Parameters: side (SideType): Dictionary representing the side's units and their stats. hits_scored (int): Number of hits to apply to the side. rng_key (RNGKey): JAX random key for generating random numbers. Returns: SideType: Updated side after applying hits. \"\"\"","title":"apply_hits"},{"location":"ti4_combat_simulator/#simulate_combat_round_jax","text":"def simulate_combat_round_jax(side_a: SideType, side_b: SideType, rng_key: RNGKey) -> Tuple[SideType, SideType]: \"\"\" Simulates a single round of combat between two sides using JAX. Parameters: side_a (SideType): First combatant side. side_b (SideType): Second combatant side. rng_key (RNGKey): JAX random key for generating random numbers. Returns: Tuple[SideType, SideType]: Updated states of side_a and side_b after combat. \"\"\"","title":"simulate_combat_round_jax"},{"location":"ti4_combat_simulator/#run_combat_until_elimination","text":"def simulate_combat_round_jax(side_a: SideType, side_b: SideType, rng_key: RNGKey) -> Tuple[SideType, SideType]: \"\"\" Simulates a single round of combat between two sides using JAX. Parameters: side_a (SideType): First combatant side. side_b (SideType): Second combatant side. rng_key (RNGKey): JAX random key for generating random numbers. Returns: Tuple[SideType, SideType]: Updated states of side_a and side_b after combat. \"\"\"","title":"run_combat_until_elimination"},{"location":"ti4_combat_simulator/#monte_carlo_combat_simulation","text":"def monte_carlo_combat_simulation(initial_side_a: SideType, initial_side_b: SideType, num_simulations: int = 1000) -> Dict[str, float]: \"\"\" Performs a Monte Carlo simulation of combat between two sides over a specified number of simulations to estimate outcome probabilities. Parameters: initial_side_a (SideType): Initial state of side A. initial_side_b (SideType): Initial state of side B. num_simulations (int): Number of simulations to run. Returns: Dict[str, float]: Probabilities of different outcomes. \"\"\"","title":"monte_carlo_combat_simulation"},{"location":"ti4_combat_simulator/#example-usage","text":"The following is an example of 2 dreadnaughts, each with 2 health hitting 60% of the time fighting vs 6 fighters, each with 1 health hitting 20% of the time. initial_side_a = {(2, 0.6): 3} initial_side_b = {(1, 0.2): 6} num_simulations = 10000 probabilities = monte_carlo_combat_simulation(initial_side_a, initial_side_b, num_simulations) print(probabilities) Experience | Education | Skills | Projects | Contact TI4 Combat Simulator | Baseball Pitch Predictor | Probabalistic Programming Tools | Data Tools","title":"Example Usage"}]}